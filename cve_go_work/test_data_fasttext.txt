Durable local storage Is there a way to pin a pod to a minion? For example we have some data that is stored on the host disk that is persistent between reboots as such I need to tell the replication controller that this container should be pinned for example to minion1 If something happens to minion1 then your pod can t run Kubernetes tries to abstract away dependency on specific minions Is it practical for you to do one of the following install the data on all hosts? make a docker image which includes this data so that it can be installed on any minion? serve it off NFS from a machine which is not a minion and then access it as a docker remote volume? I m guessing not or you wouldn t have asked but it would be helpful to understand your use case more On Thu Jul 24 2014 at 12 24 AM Mark Olliver notifications wrote Is there a way to pin a pod to a minion? For example we have some data that is stored on the host disk that is persistent between reboots as such I need to tell the replication controller that this container should be pinned for example to minion1 Reply to this email directly or view it on GitHub What Eric said We may be forced to add such constraints in the future but we re going to try hard not to We are working on additional volume types to make this easy Unfortunatly it is not that simple as the application in the docker will be updating the data all the time and we can not use NFS as that is much to much overhead for the access latencies we need Ideally in the future this data would be stored on an SSD volume mounted to the minion But for now I am happy with it being on the host but it does need to be pinned Yeah SSD access is one of the things that will probably force us to add some sort of constraint to keep your pod co located with its SSD Paging I renamed this issue to narrow it to the specific use case Support for durable local storage is an issue that has been raised by several partners in discussions and is evident in every example application we ve looked at This is a requirement for running a database other storage system SSD based cache etc Support for more types of volumes is maybe necessary but definitely not sufficient We also need to represent the storage devices as allocatable resources As I mentioned in 146 pods are currently relatively disposable and don t have durable identities So I think the main design question is this do we conflate the identity of the storage with the identity of the pod and try to increase the durability of the pod or do we represent durable local volumes as objects with an identity and lifetime independent of the pods? The latter would permit require creation of a new pod that could be attached to pre existing storage The latter is somewhat attractive but would obstruct local restarts which is desirable for high availability and bootstrapping and wouldn t interact well with replicationController due to the need to create manage an additional object and also to match individual pods and volumes which would reduce the fungibility of the pods So I m going to suggest we go with pod durability Rather than a single pin persistence bit I suggest we go with forgiveness a list of of disruption events the pod will tolerate We could support an any event type and infinite duration for pods that want to be pinned regardless of what happens This approach would generalize nicely for cases where for example applications wanted to endure reboots but give up in the case of extended outages or in the case that the disk goes bad We re also going to want to use a similar spec for availability requirements failure tolerances of sets of pods Ideally the pod could be restarted recreated by Kubelet directly This would likely require checkpointing 489 but initially we d at least need to be able to not transition the pod to a stopped state and or delete it or at least be able to recreate it with the same identity recover pre existing storage from a well known place Regarding the former we probably need to introduce some indication of outages into the pod status probably not the primary state enum but in a separate readiness field Regarding the latter there are cases where it is convenient to place the storage in the host in a user specified location to facilitate debugging data recovery etc without needing to look up long host specific system generated identifiers though that s probably not a requirement for v0 It might be nice to have a way for a durable pod to have a way to request to delete itself without making an API call Some people have suggested that run until success is not a sufficiently reliable way to convey this Perhaps we could use an empty volume on exit as the signal Certainly that would mean there wasn t any valuable data to worry about and it would be easy for an application to drop an empty file there if it just wanted to stay put Support for raw SSD should be filed as a separate issue if desired Can we start with clear statements of requirement? What we have with local volumes is already pretty durable as long as the pod stays put That may be a side effect of the implementation but maybe we should keep it Alternatively maybe the answer is Just like GCE has a PD associated with a VM we could have something similar with pods But that is getting ahead I don t feel like I really understand the required aspects of this On Thu Jul 24 2014 at 5 52 PM bgrant0607 notifications wrote I renamed this issue to narrow it to the specific use case Support for durable local storage is an issue that has been raised by several partners in discussions and is evident in every example application we ve looked at This is a requirement for running a database other storage system SSD based cache etc Support for more types of volumes is maybe necessary but definitely not sufficient We also need to represent the storage devices as allocatable resources As I mentioned in 146 pods are currently relatively disposable and don t have durable identities So I think the main design question is this do we conflate the identity of the storage with the identity of the pod and try to increase the durability of the pod or do we represent durable local volumes as objects with an identity and lifetime independent of the pods? The latter would permit require creation of a new pod that could be attached to pre existing storage The latter is somewhat attractive but would obstruct local restarts which is desirable for high availability and bootstrapping and wouldn t interact well with replicationController due to the need to create manage an additional object and also to match individual pods and volumes which would reduce the fungibility of the pods So I m going to suggest we go with pod durability Rather than a single pin persistence bit I suggest we go with forgiveness a list of of disruption events the pod will tolerate We could support an any event type and infinite duration for pods that want to be pinned regardless of what happens This approach would generalize nicely for cases where for example applications wanted to endure reboots but give up in the case of extended outages or in the case that the disk goes bad We re also going to want to use a similar spec for availability requirements failure tolerances of sets of pods Ideally the pod could be restarted recreated by Kubelet directly This would likely require checkpointing 489 but initially we d at least need to be able to not transition the pod to a stopped state and or delete it or at least be able to recreate it with the same identity recover pre existing storage from a well known place Regarding the former we probably need to introduce some indication of outages into the pod status probably not the primary state enum but in a separate readiness field Regarding the latter there are cases where it is convenient to place the storage in the host in a user specified location to facilitate debugging data recovery etc without needing to look up long host specific system generated identifiers though that s probably not a requirement for v0 It might be nice to have a way for a durable pod to have a way to request to delete itself without making an API call Some people have suggested that run until success is not a sufficiently reliable way to convey this Perhaps we could use an empty volume on exit as the signal Certainly that would mean there wasn t any valuable data to worry about and it would be easy for an application to drop an empty file there if it just wanted to stay put Support for raw SSD should be filed as a separate issue if desired Reply to this email directly or view it on GitHub The durable pod described above matches our experience with a broad range of real world use cases many organizations are willing to support reasonable durability of containers in bulk as long as the operational characteristics are understood ahead of time They eventually want to move applications to more stateless models but accepting outages and focusing on mean time to recovery is a model they already tolerate Furthermore this allows operators to focus on durability in bulk with a corresponding reduction in effort over their previous single use systems We d be willing to describe a clear requirement for a way to indicate that certain pods should tolerate disruption with a best effort attempt to preserve local volumes and the container until such a time as the operator describes a host The suggestion to indicate a pod is done by clearing its storage is elegant although in practice it s either user intervention or the container idling out of use User specified data locations is also not significant for us in the near term 1 to the forgiveness model Let s make sure that it s possible to list the same reason multiple times we d like to make it possible to forgive a few long outages and many shorter ones john On Thu Jul 24 2014 at 5 52 PM bgrant0607 notifications wrote I renamed this issue to narrow it to the specific use case Support for durable local storage is an issue that has been raised by several partners in discussions and is evident in every example application we ve looked at This is a requirement for running a database other storage system SSD based cache etc Support for more types of volumes is maybe necessary but definitely not sufficient We also need to represent the storage devices as allocatable resources As I mentioned in 146 pods are currently relatively disposable and don t have durable identities So I think the main design question is this do we conflate the identity of the storage with the identity of the pod and try to increase the durability of the pod or do we represent durable local volumes as objects with an identity and lifetime independent of the pods? The latter would permit require creation of a new pod that could be attached to pre existing storage The latter is somewhat attractive but would obstruct local restarts which is desirable for high availability and bootstrapping and wouldn t interact well with replicationController due to the need to create manage an additional object and also to match individual pods and volumes which would reduce the fungibility of the pods So I m going to suggest we go with pod durability Rather than a single pin persistence bit I suggest we go with forgiveness a list of of disruption events the pod will tolerate We could support an any event type and infinite duration for pods that want to be pinned regardless of what happens This approach would generalize nicely for cases where for example applications wanted to endure reboots but give up in the case of extended outages or in the case that the disk goes bad We re also going to want to use a similar spec for availability requirements failure tolerances of sets of pods Ideally the pod could be restarted recreated by Kubelet directly This would likely require checkpointing 489 but initially we d at least need to be able to not transition the pod to a stopped state and or delete it or at least be able to recreate it with the same identity recover pre existing storage from a well known place Regarding the former we probably need to introduce some indication of outages into the pod status probably not the primary state enum but in a separate readiness field Regarding the latter there are cases where it is convenient to place the storage in the host in a user specified location to facilitate debugging data recovery etc without needing to look up long host specific system generated identifiers though that s probably not a requirement for v0 It might be nice to have a way for a durable pod to have a way to request to delete itself without making an API call Some people have suggested that run until success is not a sufficiently reliable way to convey this Perhaps we could use an empty volume on exit as the signal Certainly that would mean there wasn t any valuable data to worry about and it would be easy for an application to drop an empty file there if it just wanted to stay put Support for raw SSD should be filed as a separate issue if desired Reply to this email directly or view it on GitHub What happens today in the following scenarios Kubelet dies Docker daemon dies Host reboots Host is down unreachable for 15 minutes For a regular pod without a replication controller absolutely nothing for the rep controller case except for dockerd death they result in a new pod being spun up somewhere And then if the old pod shows up again one of the pods will get killed Not sure exactly what a dockerd death would cause Dockerd death leaves orphaned processes that the daemon doesn t know is running EDIT corrected right now children processes stay running until the daemon starts at which point the daemon loops over all containers and kills them and then will not restart them unless daemon AutoRestart is true On Thu Jul 24 2014 at 10 36 PM bgrant0607 notifications wrote What happens today in the following scenarios Kubelet dies Nothing happens Docker daemon dies All containers die kubelet probably craps itself trying to talk to docker daemon What should happen is that containers should stay alive but kubelet can t talk to dockerd Host reboots Unless someone moves the pod from that host the pod comes back with the host Host is down unreachable for 15 minutes Nothing unless someone moves the pod in etcd What do we want to happen? I think host pinning and forgiveness stickiness is going to be unavoidable for some use cases There s a difference between pinning and stickiness though Pinning implies that you know what host you want whereas forgiveness says On Thu Jul 24 2014 at 3 07 PM Daniel Smith notifications wrote Paging Reply to this email directly or view it on GitHub Related to this is the need for hooks for supporting the migration of stateful containers Moving stateless containers is easy you start a new one on a different host and kill the old one But with stateful containers pods it almost always takes custom scripting e g simple rsync for files or bringing up a DB container as a slave before promoting it to be the new Master or whatever else is needed Kyle Mathews Blog Twitter On Sat Jul 26 2014 at 8 57 AM Tim Hockin notifications wrote I think host pinning and forgiveness stickiness is going to be unavoidable for some use cases There s a difference between pinning and stickiness though Pinning implies that you know what host you want whereas forgiveness says On Thu Jul 24 2014 at 3 07 PM Daniel Smith notifications wrote Paging Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Pinning to a specific host could be achieved either using constraints or the forthcoming direct scheduling API in addition to forgiveness I usually agree with but I m going to explore a contrary position on this issue Takes attention away from Stateless Process model The model of building systems of stateless processes is a powerful one Making pods possibly persistent gives the appearance of controverting that model Punts on issue of recreating data If an object is going to be managed to a system like k8s then it should be possible for the system to recreate the object without user intervention Once a k8s cluster and pods and controllers are started it should be able to continue working approximately forever despite a low rate of hardware failures and repairs Therefore you have to be able to recreate any particular local chunk of data Until we have a way to achieve non zero availability forever it is too soon to start optimizing for better availability Once we have a way to specify how to create a data object we may find that there is enough information in the specification it self to infer what the should be For example if the requirements for a data item are 1TB then we should be default forgive for outages that are shorter than the mean time to create a 1TB data item Forgiveness too sophisticated for most users Forgiveness is well defined for reasoning about availability from first principles However my experience is that even users with mature development processes don t reason about availability from first principles they take an iterative approach only adjusting their systems in response to problems they actually experience We should not add a complex feature that may not be necessary It will certainly complicate writing additional schedulers and make the pod specifications more intimidating Some thoughts Is the goal of a system like kubernetes to run only 12 factor applications? Are the benefits of introducing stateful software to a flexible container infrastructure sufficient to outweigh the minor accommodations that they require? Is creating a replication controller today implicitly correlated to creating stateless software? An assumption I had been working for is that replication controller is the abstraction that provides the illusion of recreating an object without user intervention That the scheduler does not reschedule containers instead the reconciliation loop of the replication controller forms dynamic tension with a scheduler by deleting containers that no longer fit an appropriate definition of health Thus the schedulers responsibility is reduced it only attempts to place new pods but never reschedules In this model the scheduler does not need to know about forgiveness rather the replication controller does And the replication controller is the one that needs to make the decision about health If that s not the case then where does that responsibility lie? Would the scheduler be responsible for deleting pods off one location placing them on another and determining whether that transition is appropriate? If so that seems like a growth in responsibility of the scheduler every scheduler would then need to deal with the complexity of knowing and hardcoding the list of transitions The former model seems more flexible for instance replication controller types can be created of arbitrary complexity including forgiveness with each replication controller needs to deal with the consequences of when delete is appropriate And ultimately determining when something should be deleted is often specific to use case I wasn t suggesting that we should not support stateful apps just that we should not support stateful apps with only the Pod object Brian said earlier do we conflate the identity of the storage with the identity of the pod and try to increase the durability of the pod or do we represent durable local volumes as objects with an identity and lifetime independent of the pods? I was arguing for the second option that we have a different object type to represent that persistent data Those data objects would need their own replication control data could be more widely replicated than Pods and could be in different states I can at least talk to running moderately dense container hosts with a single shared durable storage volume per host The storage is network attached and snapshot able for backup purposes For most outages the volume was detached and reattached to a new host with the same identity as the old host with the containers not rescheduled This has been a reasonable solution for most operation teams running OpenShift trading off some availability for reduced complexity of managing that a single volume during recovery And in those types of outages the most current and accurate data for those containers is on the volume so replication is unlikely to be faster than restoring the volume This is just one particular scenario but it s a sort of local minima of availability for stateful containers at reasonable density and familiarity to ops teams And in this case forgiveness does seem to model the tradeoff better waiting longer before deciding the state is gone However to your point if that particular volume is never coming back having a well set up model for distributing state and tracking independent volumes reduces your vulnerability to total loss Also the planned reallocation model works better along your model if I decide to evacuate a host for maintenance I may very well want to rebalance to other hosts and that requires a certain volume with a certain set of data in place on that other host Revisiting older topics that I think are important This tapered off with no clear resolution I still don t feel like I understand the behavioral requirements We ve discussed a lot of considerations of a couple of implementations but have not discussed exactly what we are trying to achieve Are we trying to enable data objects to have a lifetime that is decoupled from any one pod? Are we trying to allow pods to have large data ? other? I think data objects decoupled from pod is modeled with sufficient granularity in volumes today Being able to define some level of pod stability that does not cause significant scheduling difficulties has value for places where large local data exists Perhaps this belongs as a scheduler problem where an integrator can determine that a volume type as a corresponding impact on scheduling decisions Volumes have a lifetime coupled perfectly their pod If we are arguing that there s a need to have durable data that outlives any pod we have not really started that design I think a major use case for this is Pod software upgrades Right now upgrading software deployed inside Containers in a Pod is afaict a destructive operation Proposal 1 Define a VolumeCache resource 1 A VolumeCache can be manually created or it can be created by a controller which makes the number of VolumeCaches for a corresponding PodTemplate 2 VolumeCaches have a lifetime independent from a Volume or Pod 3 When a pod is restarted the VolumeCache object remains bound and the VolumeCache s data remains on the local storage media until the VolumeCache object is manually deleted or deleted by the controller 4 A VolumeCache is on a specific machine 5 When the data itself is lost then the VolumeCache object is unbound from the Minion and deleted The kubelet could report the loss of a Volume e g if the SSD fails 6 VolumeCaches can request certain amounts of local storage 7 VolumeCache states 1 Allocated 2 Ready 3 InUse 4 Corrupt 2 Relation between Pods and Volumes and VolumeCaches 1 A Pod can ask for Volumes 2 Volumes live as long as a Pod 3 Some types of Volumes require a VolumeCache to function Four types of Volumes 1 Ephemeral local storage with lifetime same as Pod No VolumeCache Pods that only use Ephemeral Volumes or no Volumes are truly stateless 2 Remote volume which gives access to remote storage such as GCE PD No VolumeCache Pods that use a Remote volume are not stateless They can potentially get into a state where they have invalid data in their Remote requiring operator intervention 3 Cached volume which uses a VolumeCache A Pod which uses Cached volumes but not Remote is Stateless from a correctness standpoint but requires state from a performance standpoint 4 A Pod that needs a Volume that needs a VolumeCache can only be scheduled on a Minion which already has a VolumeCache of the right sort 5 When a pod is restarted the VolumeCache represents the copy of the data that the Pod s Volume needs The scheduler will naturally try to restart the Pod on the minion that has the VolumeCache 6 When a pod is bound to a machine with an existing VolumeCache that it needs it starts right up 3 Control strategies 1 Manually place VolumeCache on one or few minions e g the ones with the SSDs Don t ever delete them However if all the VolumeCaches are unbound due to the Minions disappearing or hardware failures then the Pods go pending Manually set up the initial data in the VolumeCaches using some mechanism outside Kubernetes Then manually advance the state to Ready Then pods can start 2 A controller defines how to initially populate a VolumeCache by copying its data from some other location This is to be avoided but can be done at bootstrap or an emergency The controller creates the right number of VolumeCaches waits for them to be Allocated then runs the population command And then makes them Ready If too many become Corrupt it makes new ones If it is autoscaling pods then it may make extra VolumeCaches to allow for predicted autoscaling needs without using Pod resources until needed Maybe is a better name than wrote above I think this proposal addresses those concerns models reality avoids introducing an abstract concept Brian s concerns local restarts the kubelet will know the state of a VolumeCache and know about a Volume s dependence on it and so can reason about whether a local restart is possible interact with replicationController the replicationController or a new type of controller could manage both pods and VolumeCaches fungibility of pods A Pod can be bound to any minion with a compatible VolumeCache See 1515 Things I learned in the discussion at Kubercon it is typical practice to run databases on dedicated machines VMs from frontend processes So using dedicated nodes for database pods will be familiar for users many SQL and NoSQL databases don t have automatic failover master election if the master db fails So users would not gain anything from using replication controllers They need to manually handle node failures at least mysql and perhaps for other databases the database s on disk storage is not identical for master process vs slave process So you can t mix up a pod with the master flag with a data volume that previously ran a slave or vice versa So again users won t benefit from replication controllers for database pods  
Consider not handling YAML in APIserver The APIserver processes JSON and YAML A vulnerability in the apiserver would be critical Compared to JSON YAML has a crazy complicated SPEC YAML has a go implementation that is more complex 14KLOC JSON vs 30KLOC YAML YAML has fewer tests in the go implementation 8KLOC JSON vs 2 5KLOC YAML Has a track record of vulnerabilities YAML has fewer vulnerabilities but this may be due to its less widespread use in critical software 24 CVEs match YAML vs 66 JSON using Therefore we should consider one of the following changes 1 Do not process YAML in the apiserver itself 1 Convert YAML to JSON in clients 2 Recommend that users needing to use YAML via etc can pipe YAML through simple yaml to json converter tool 2 Convert YAML to JSON in a less privileged proxy process Also this would eliminate the minor annoyance of having to repeat in go struct tags I could be persuaded Although go doesn t have buffer overruns so it s a little hard to see how you could get a security exploit with malicious yaml One way would be if the YAML library implementer added extensions that have side effects something analogous to either of these two vulnerabilities To be fair I m not aware of any such extension in the goyaml library but I have a sense that such craziness is more likely to appear in a YAML parser than in the Golang built in json parser package Those are both pretty awesome but I think they both depend on the interpreted language I believe that class of exploits isn t possible in go I think Go is safe from those because the go parser cannot have side effects that affect execution of other types As long as our objects are purely value objects the risk is low If we introduced method calls on deserialized objects it might be possible for someone to exploit an interface style declaration with an arbitrary type One caveat the types in pkg api types go are used in the kubelet and the master Kubelet does rely on being able to parse YAML from config files and manifests That doesn t mean the server has to allow YAML but it does mean that we have to keep supporting it On Fri Aug 22 2014 at 5 43 PM Clayton Coleman notifications wrote I think Go is safe from those because the go parser cannot have side effects that affect execution of other types As long as our objects are purely value objects the risk is low If we introduced method calls on deserialized objects it might be possible for someone to exploit an interface style declaration with an arbitrary type Reply to this email directly or view it on GitHub Kubecfg as well This is actually something we can evolve Codec to support anyway there might just be a YAML2JSONCodec On Aug 23 2014 at 2 02 AM Tim Hockin notifications wrote One caveat the types in pkg api types go are used in the kubelet and the master Kubelet does rely on being able to parse YAML from config files and manifests That doesn t mean the server has to allow YAML but it does mean that we have to keep supporting it On Fri Aug 22 2014 at 5 43 PM Clayton Coleman notifications wrote I think Go is safe from those because the go parser cannot have side effects that affect execution of other types As long as our objects are purely value objects the risk is low If we introduced method calls on deserialized objects it might be possible for someone to exploit an interface style declaration with an arbitrary type Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub I m 1 on this because of the simplicity of only supporting one format in the API 1 from me also I like keeping the API simpler YAML in an API is rare YAML in Kubecfg is not a concern because it is not running as and the invoker typically controls the yaml file On Mon Aug 25 2014 at 10 27 AM Joe Beda notifications wrote 1 from me also I like keeping the API simpler YAML in an API is rare Reply to this email directly or view it on GitHub We initially supported only JSON in the container manifest Then we added YAML because it is easier to write and read Are we talking about backing out of YAML for container VM? On Mon Aug 25 2014 at 1 55 PM erictune notifications wrote YAML in Kubecfg is not a concern because it is not running as and the invoker typically controls the yaml file On Mon Aug 25 2014 at 10 27 AM Joe Beda notifications wrote 1 from me also I like keeping the API simpler YAML in an API is rare Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub The reason for having YAML in the manifest was to make it easier to write them by hand For cases where people are using the kubecfg client it is easy enough to translate the request in the client As our config story gets clearer I think there will be fewer needs to hand craft manifests and fewer needs to send them without using a client program However for we should continue to support YAML manifests for continuity It is not really a security issue since the customer also owns the VM For kubernetes on github I think that it should be impossible in the master and default disallowed in the kubelet Tim suggested an allow yaml flag to kubelet to be used in the containervm case Fuel to the fire I was looking at using net IP in the API objects It works great with JSON but obviously does not support YAML Decode uses YAML logic which means IPs do not decode Using a type alias with a couple custom methods can be assigned from a net IP but breaks because the methods do not Using an embedded type inherits methods but can not be assigned from a net IP Sigh Go Short of implementing our own IP type that implement the interfaces we need as simple forwarders we can t use net IP in our API objects Unless we drop YAML On Tue Aug 26 2014 at 4 21 PM erictune notifications wrote The reason for having YAML in the manifest was to make it easier to write them by hand For cases where people are using the kubecfg client it is easy enough to translate the request in the client As our config story gets clearer I think there will be fewer needs to hand craft manifests and fewer needs to send them without using a client program However for containervms we should continue to support YAML manifests for continuity It is not really a security issue since the customer also owns the VM For kubernetes on github I think that it should be impossible in the master and default disallowed in the kubelet Tim suggested an allow yaml flag to kubelet to be used in the containervm case Reply to this email directly or view it on GitHub Here s an idea Drop yaml Even in the client where we do want to read yaml implement it by un yaml ing into a map interface and use reflection to copy that into our api types This way we can lose all the yaml tags forever The only thing that really needs YAML is the kubelet reading from config files On Wed Aug 27 2014 at 11 49 PM Daniel Smith notifications wrote Here s an idea Drop yaml Even in the client where we do want to read yaml implement it by un yaml ing into a map interface and use reflection to copy that into our api types This way we can lose all the yaml tags forever Reply to this email directly or view it on GitHub Ah yes where I said read Is there any conclusion to this? It doesn t seem like anyone is strongly objecting dropping YAML and it would simplify many things Kubecfg must be capable of taking comfig files in yaml It is simply more human editable than json and that s important I don t care if we do the translation client side though Brendan On Sep 27 2014 11 36 AM notifications wrote Is there any conclusion to this? It doesn t seem like anyone is strongly objecting dropping YAML and it would simplify many things Reply to this email directly or view it on GitHub We can expose more codecs than we do today For instance we can introduce a new Codec wrapper that supports Decode from YAML as the first step and then feeds that to the existing Codec Eventually Decode and Encode should be split for most use cases they are not the same patterns and we in general could live with one decoder but many encoders On Sep 27 2014 at 3 29 PM Brendan Burns notifications wrote Kubecfg must be capable of taking comfig files in yaml It is simply more human editable than json and that s important I don t care if we do the translation client side though Brendan On Sep 27 2014 11 36 AM notifications wrote Is there any conclusion to this? It doesn t seem like anyone is strongly objecting dropping YAML and it would simplify many things Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub I think no matter what kubecfg has to take YAML and kubelet has to take YAML That means we still need YAML encode decode support for anything in api types go On Sat Sep 27 2014 at 12 48 PM Clayton Coleman notifications wrote We can expose more codecs than we do today For instance we can introduce a new Codec wrapper that supports Decode from YAML as the first step and then feeds that to the existing Codec Eventually Decode and Encode should be split for most use cases they are not the same patterns and we in general could live with one decoder but many encoders On Sep 27 2014 at 3 29 PM Brendan Burns notifications wrote Kubecfg must be capable of taking comfig files in yaml It is simply more human editable than json and that s important I don t care if we do the translation client side though Brendan On Sep 27 2014 11 36 AM notifications wrote Is there any conclusion to this? It doesn t seem like anyone is strongly objecting dropping YAML and it would simplify many things Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Kubecfg could convert YAML to json in a schema agnostic unvalidated manner And then send json to APIserver for handling Kubelet could do the same perhaps using a command line tool to do the YAML json Something like 1 just a general util ToJSON that takes yaml or json text and makes sure it s safe for JSON would be ideal Original Message Kubecfg could convert YAML to json in a schema agnostic unvalidated manner And then send json to APIserver for handling Kubelet could do the same perhaps using a command line tool to do the YAML json Something like Reply to this email directly or view it on GitHub More arguments in favor of dropping YAML in the API 1 No official standard mime type 2 Not supported by 3 Not supported by any API doc browser that I m aware of When building the config generation transformation tools it was useful to have YAML decoding encoding support but I suppose these could be redone in terms of JSON with a yaml2json prepass assuming that feeding the pass valid json would work Ideally every config client tool would be able to accept and produce either yaml or json but we d then always convert to json before sending the data to the API How much of a problem is it to keep the yaml field tags? Very little to keep the tags Original Message More arguments in favor of dropping YAML in the API 1 No official standard mime type 2 Not supported by 3 Not supported by any API doc browser that I m aware of When building the config generation transformation tools it was useful to have YAML decoding encoding support but I suppose these could be redone in terms of JSON with a yaml2json prepass assuming that feeding the pass valid json would work Ideally every config client tool would be able to accept and produce either yaml or json but we d then always convert to json before sending the data to the API How much of a problem is it to keep the yaml field tags? Reply to this email directly or view it on GitHub The number of bugs that found in yaml v2 mentioned in 2600 supports the original premise that yaml is too complex for something security critical like the apiserver We ve removed the yaml tags from types go files I d like to go ahead and enforce Content Type to be json now However AFAICT our client library doesn t set Content Type anywhere Anyone have a preference where in our many layers of client libraries this should be set? Implemented as optional in and enabled by Makes sense to me We have a few cases where we set different on server We may want to qualify the content type for watch On Dec 16 2014 at 9 32 PM bgrant0607 notifications wrote We ve removed the yaml tags from types go files I d like to go ahead and enforce Content Type to be json now However AFAICT our client library doesn t set Content Type anywhere Anyone have a preference where in our many layers of client libraries this should be set? Implemented as optional in and enabled by w Header Set Reply to this email directly or view it on GitHub On Dec 16 2014 6 54 PM notifications wrote Makes sense to me We have a few cases where we set different on server We may want to qualify the content type for watch On Dec 16 2014 at 9 32 PM bgrant0607 notifications wrote We ve removed the yaml tags from types go files I d like to go ahead and enforce Content Type to be json now However AFAICT our client library doesn t set Content Type anywhere Anyone have a preference where in our many layers of client libraries this should be set? Implemented as optional in and enabled by w Header Set Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Sorry but what are you proposing? cc  
Volumes are created in container with root ownership and strict permissions The emptyDir volumeMount is owned by root root and permissions set to 750 hostDir is the same but with 755 permissions Containers running with a non root USER can t access the volumes Related discussion at and Docker issue hostDir should get the same permissions as the existing host entry though I am not sure we ensure a host direct exists before using hostDir Part of the problem here is that different containers can run as different users in the same pod which user do we create the volume with? what we really need is a way to tell docker to add supplemental group IDs when launching a container so we can assign all containers in a pod to a common group I filed Would it be reasonable to add and or option to or to explicitly force it? I don t think that we want that in the API long term so I d rather apply a hidden heuristic like or even Do you think such heuristics would hold? On Wed Nov 26 2014 at 9 07 AM Carlos Sanchez notifications wrote Would it be reasonable to add user and or permissions option to volumeMounts or emptyDir to explicitly force it? Reply to this email directly or view it on GitHub That sounds good to me This is a good starter project Inside a docker container the primary process is launched as root by default And currently docker containers can not be run without root privileges However even today inside a docker container a process can be run under a non privileged user the Docker image can create new users and then force docker to launch the entry point process as that user instead of root When an external volume is mounted it s permissions are set to ROOT therefore unless the process inside the container is launched as root it won t have permission to access the mounted directory 1 While creating pod if it requires an EmptyDir volume before starting containers retrieve the USER from each container image if any of the containers are launching their main process as non root fail pod creation 2 While creating pod if it requires an EmptyDir volume before creating shared volume Chown it to the USER of the first container that mounts the volume Problems with this approach 1 With Kubernetes a pod can contain multiple containers that share a volume but each container could potentially run their processes with different users inside meaning even if the owner of a volume was changed unless the owner was changed to a group that all containers were aware of the problem would still exist Not that big a deal because we could handle it with docker docker 9360 2 Another interesting dimension to the problem is that running CHOWN on a shared volume from outside the containers could fail if the host machine does not have the same user as inside the container One work around for this is to share the etc passwd file between the host and the container but that is very limiting Another potential workaround would be for the host to some how reach inside the container during initialization read the USER that the main process will start with and use the image etc passwd file to map the USER to UID and CHOWN the shared volume on the host to that UID Both approaches feel to me like they are breaking a layer of abstraction by having Kubernetes reach into the container to figure out what user the main process would start as and doing something outside the container with that information I feel like the right approach would be for the containers themselves to CHOWN any during setup Thoughts? after talking to some folks I think s approach of explicitly specifying the user in the API would be the cleanest work around I don t thing we can apply without doing icky violation of abstractions Proposal to modify the API Extend the API for and volumes to optionally specify a unsigned integer UID If the UID is specified the host will change the owner of the directory to that UID and set the permissions to when the volume directory is created If the UID is not specified the host will not change the owner but set the permissions to i e world writable when the volume directory is created HostDir volumes would be left untouched since those directories are not created by Kubernetes Require UID instead of username string so there are no problems if the user does exist on the host machine Thoughts? CC I think adding UID to volumes is a hack and redundant I d rather we do the right thing and get Docker to support supplemental group IDs On Thu Dec 18 2014 at 3 13 PM Saad Ali notifications wrote after talking to some folks I think approach of explicitly specifying the user in the API would be the cleanest work around I don t thing we can apply without doing icky violation of abstractions Proposal to modify the API Extend the API for EmptyDir GitRepo and GCEPersistentDisk volumes to optionally specify a unsigned integer UID If the UID is specified the host will change the owner of the directory to that UID and set the permissions to 750 when the volume directory is created If the UID is not specified the host will not change the owner but set the permissions to 757 i e world writable when the volume directory is created HostDir volumes would be left untouched since those directories are not created by Kubernetes Require UID instead of username string so there are no problems if the user does exist on the host machine Thoughts? CC Reply to this email directly or view it on GitHub ali I think HostDir should not be left untouched Let s consider this Hadoop on restart restores the blocks from the directory it stores data in If we use emptyDir the container which restarted will get another directory and the previous data will be lost And Hadoop requires the permissions and ownership of directory to be set to the user starting Hadoop If HostDir is not allowed to change permissions as per user then similar use cases to this cannot be achieved Please comment Define restart? Do you mean the container crashed and came back or do you mean the machine rebooted and a new pod was scheduled and expects to be able to reclaim the disk space used by the previous pod? Or something else? On Mon Jan 12 2015 at 11 56 PM Luqman notifications wrote ali I think HostDir should not be left untouched Let s consider this Hadoop on restart restores the blocks from the directory it stores data in If we use emptyDir the container which restarted will get another directory and the previous data will be lost And Hadoop requires the permissions and ownership of directory to be set to the user starting Hadoop If HostDir is not allowed to change permissions per user then similar use cases to this cannot be achieved Please comment Reply to this email directly or view it on GitHub Restart could be anything It could be after pod failure or container failure Or the container could be restarted after changing some configurations Does that answer? This mentions that when a pod is unbound the emptyDir is deleted In use case of Hadoop the data might be essential and might be required when another pod of Hadoop comes back So HostDir must be used to persist data even the pod is unbound But Hadoop requires permissions to be set for the user for the data directory Hope this explains With docker libcontainer pull 322 docker containers now allow specifying So an updated proposal to handle shared volumes amongst different containers in a pod When creating or volumes for a new pod Kubelet will 1 Create a new linux group for the pod on the host machine Group is created with the next available Group ID number 2 Change the group of the new directory to the newly created group 3 Set the permissions of the new directory to 4 For each docker container pass in the GID of the new group as via docker container configs Still requires docker to support passing through to libcontainer May require updating to support When creating volumes for a new pod Kubelet will Leave the volume untouched since those directories are not created by Kubernetes this is up for debate but my thinking is that since Kubernetes does not create the HostDir and since it may contain existing data Kubernetes should not get in to the business of modifying it We should leave it up to the creator and maintainer of the HostDir to modify it s ownership or permissions to allow containers to access it There s an important distinction between a container restarting and a pod being removed When a container restarts the data in a normal emptyDir volume is safe when a pod is removed it should be GONE Leaving Hostdata and expecting it to be there at some later point in time is awkward at best All of this is more complicated as soon as user namespaces land On Wed Jan 14 2015 at 12 04 AM Luqman notifications wrote This document mentions that when a pod unbound the emptyDir is deleted In use case of Hadoop the data might be essential and might be required when another pod of Hadoop comes back So HostDir must be used to persist data even the pod is unbound But Hadoop requires permissions to be set for the user for the data directory Hope this explains Reply to this email directly or view it on GitHub If I m understanding the current proposal correctly I think this is going to create surprising behavior for a certain class of applications Many older applications which bind to low ports start first as root then immediately drop privileges to some other user In such a scenario the container must be configured to start the application as root and so the original user would have access to the volume Once the application calls though it won t have access anymore Now the only way to get access to that directory is to modify the container to the volume before starting the application itself This is the situation I m currently in Due to this I d like to voice another opinion in favor of extending the API to allow explicitly specifying and as I don t think the current proposal covers all possible use cases At a minimum the emptydir should use the UID GID Labels of the security context I think adding UID to volumes is a hack and redundant I d rather we do the right thing and get Docker to support supplemental group IDs 1 but also At a minimum the emptydir should use the UID GID Labels of the security context Now that we have security context API in place I think we should make emptyDir work with the security context of the containers in a pod One little wrinkle to iron out about this is that volumes are pod scoped while security contexts are container scoped I think you will have to look at which containers have the volume mounted and what their security contexts are If there s a single container that mounts an emptyDir it s easy use the security context of that container If there are multiple it gets dicey 1 Do all containers mounting the emptyDir need to have the same UID? 2 Do the containers have to have the exact same SELinux options? 3 Can we have use cases where there are two different SELinux contexts that both need to use the volume? Can we synthesize the levels for a volume from the labels of different SELinux contexts of different containers that have the same user role and type but different levels? I think I will probably start prototyping this concentrating on the simple case where there s a single container I will use the security context of the first container that mounts the volume in the pod spec at first and we can change the strategy for determining the context to use as discussion goes I don t really like the heuristics here I acknowledge that I suggested a heuristic but that was half a year ago Other than Docker support not being done yet why can t we do something like 1 All volumes get allocated a unique GID 2 All volumes are mounted g srwx 3 All containers in a pod get all volumes in that pod s GIDs in their supplemental groups Net result should be that all containers in the pod can access all volumes in the pod without restriction regardless of what UID each container is using I don t know anything about SELinux labels might be a problem I assert that all volumes in a pod should be available to all containers Next we have to define emptyDir is pretty obvious What happens to hostPath mounts that did not exist and were created for this use? Seems reasonable What about hostPath mounts that existed before this use no way can we change those What about things like PDs? Do we run a recursive chown chgrp chmod Blech for docker support status on supplemental groups Waiting for to be merged I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub I don t find the case of two containers in a pod needing different access control to a volume to be very compelling The alternative is to spec a full security context for volumes and then force the complexity back onto API users On Thu Jun 4 2015 at 9 30 AM Clayton Coleman notifications wrote Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub The argument here seems to be that you don t need intra pod security more complex than I m ok with a single security context for the pod just pointing out that if you want to have complex secure pods you may want to use user isolation between the containers to secure disk contents Original Message I don t find the case of two containers in a pod needing different access control to a volume to be very compelling The alternative is to spec a full security context for volumes and then force the complexity back onto API users On Thu Jun 4 2015 at 9 30 AM Clayton Coleman notifications wrote Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Hrm Although the pod security context is unlikely to work for most real containers once user namespaces land the UID a container runs as is really tied to the container not the pod So either that has to be a default security context at the pod level or it s instead the volume security context Original Message The argument here seems to be that you don t need intra pod security more complex than I m ok with a single security context for the pod just pointing out that if you want to have complex secure pods you may want to use user isolation between the containers to secure disk contents Original Message I don t find the case of two containers in a pod needing different access control to a volume to be very compelling The alternative is to spec a full security context for volumes and then force the complexity back onto API users On Thu Jun 4 2015 at 9 30 AM Clayton Coleman notifications wrote Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub We should share user namespace across the pod On Thu Jun 4 2015 at 10 32 AM Clayton Coleman notifications wrote Although the pod security context is unlikely to work for most real containers once user namespaces land the UID a container runs as is really tied to the container not the pod So either that has to be a default security context at the pod level or it s instead the volume security context Original Message The argument here seems to be that you don t need intra pod security more complex than I m ok with a single security context for the pod just pointing out that if you want to have complex secure pods you may want to use user isolation between the containers to secure disk contents Original Message I don t find the case of two containers in a pod needing different access control to a volume to be very compelling The alternative is to spec a full security context for volumes and then force the complexity back onto API users On Thu Jun 4 2015 at 9 30 AM Clayton Coleman notifications wrote Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub The user namespace of the two containers should probably be in the same range But the UID of container A and B are not required to be and in many cases you don t want them to be trivially because you may want to the volume but not write it Original Message We should share user namespace across the pod On Thu Jun 4 2015 at 10 32 AM Clayton Coleman notifications wrote Although the pod security context is unlikely to work for most real containers once user namespaces land the UID a container runs as is really tied to the container not the pod So either that has to be a default security context at the pod level or it s instead the volume security context Original Message The argument here seems to be that you don t need intra pod security more complex than I m ok with a single security context for the pod just pointing out that if you want to have complex secure pods you may want to use user isolation between the containers to secure disk contents Original Message I don t find the case of two containers in a pod needing different access control to a volume to be very compelling The alternative is to spec a full security context for volumes and then force the complexity back onto API users On Thu Jun 4 2015 at 9 30 AM Clayton Coleman notifications wrote Although I may collocate two containers and not want them to share contents but share a work for On Jun 4 2015 at 12 22 PM Tim Hockin notifications wrote I don t know if that means we need a pod level security context or something though On Thu Jun 4 2015 at 8 40 AM Paul Morie notifications wrote I assert that all volumes in a pod should be available to all containers Spent a lot of time thinking about this last night and I agree with you on this point Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub Reply to this email directly or view it on GitHub The user namespace of the two containers should probably be in the same range But the UID of container A and B are not required to be and in many cases you don t want them to be trivially because you may want to the volume but not write it Do you think we could infer this by whether the readOnly flag is set on the VolumeMount? Some thoughts 1 When mounting a volume for a container it could inherit the SC of the container if it has nothing set 2 For a complex case we could spec out the SC for the volume to support ranges of SELinux labels as mentioned before and in this case it would not inherit the SC of the volume 3 For a predefined volume SCs the container s SC would need to be allocated in a manner consistent with the desired security to facilitate custom complex approaches with fine grained access  
What storage driver should AWS use? I have struggled to find a filesystem kernel for AWS that really works well GCE uses wheezy with a custom kernel and aufs but aufs seems like a dead end it isn t in the kernel and looks like it never will be AFAICT Getting GCE s kernel onto AWS would likely be work both one off and ongoing overlayfs seems like the future of aufs but is only in newer kernels if we re going to use a newer kernel btrfs has a lot of benefits if it works It is definitely more reliable in newer kernels I never got devicemapper to work It apparently needs a custom build of Docker I think the actual OS is driven more by the kernel Ubuntu seems to make it a bit easier to run newer longterm kernels so I ve been using that Ubuntu with btrfs seems to work pretty well although I still hit the disk full problem which was supposed to be fixed in 3 18 Btrfs looks like it has some benefits in the desktop and server scenarios However the benefits appear reduced in the Kubernetes environment Atomic COW snapshots this is useful for backups but you shouldn t need to backup your node because it can be recreated Per block checksumming nice but again nodes can be reimaged or replaced if they start acting weird persistent volumes are backed by a cluster file system that already has software level reduncancy Volume management what are partitions for when you have images? I d opt for stability over features I probably don t need CoreOS defaults to ext4 overlayfs which gets us away from btrfs But I think there is some work needed to get the master running on CoreOS I have done a lot of investigation and here is what I have found Ubuntu 14 04 images include a fairly old kernel Lots of e2e tests failures with aufs enabled They mostly pass with btrfs but the older kernel version has issues with the disk being full when it is not Debian Wheezy images have a kernel that is just too old Docker doesn t officially support it Debian Jessie images have a newer kernel and this would be a good option but you have to set some boot options and reboot Vivid Vervet seems to work pretty well no reboot needed but it uses systemd and our salt version is super buggy with systemd at least on 15 04 These Salt problems do not appear to have been fully resolved But if we can get past the Salt problems it looks like a fairly good choice aufs overlayfs btrfs look like they will all work The options are Vivid Vervet with some salt work or 14 04 Wheezy Jessie upgrade the kernel boot options with either a reboot or a custom AMI Rebooting is painful both for users and for our e2e tests Our scripts don t do particularly well with a reboot in the middle though they largely tolerate it I think the most sensible thing would be to reboot just before starting salt and rely on salt being configured to auto start on next boot This does then mean that the kernel upgrade would have to be done outside of salt Making our own AMI is not fun either particularly because then someone is then on the hook for upgrading it every time they find a vulnerability in overlayfs I propose to make Vivid Vervet the default for Kubernetes Version 1 on AWS we ve been running with Ubuntu anyway as the default I ll add a kernel upgrade with reboot to the other platforms if I have time but I don t think it is critical path There s some Salt work for Vervet but I have it sitting in a branch that I will tidy up and it works great if I essentially pull systemd management out of salt I personally favor btrfs I agree that we don t need the benefits but I think containers are based around volumes sharing files and btrfs gets this right with snapshots and copy on write files I agree entirely that we shouldn t care about backups here My view is that checksumming is particularly useful if you can just panic and throw away the container machine and that volume management is really useful on EC2 which for some reason decides to present its instance storage as multiple devices it is much more convenient to combine them using btrfs than it is to do LVM as well But I think with vivid vervet the storage mechanism can be the user s choice We can even make aufs the default so it s closer to GCE CoreOS works well on AWS with k8s Bootstrapping the cluster via cloud config is quite simple and eliminates the need for salt and opaque automation scrips  
Make it possible to shell into any container even if it has no shell Today depends on a shell being present in each container image I think we can do better What if we had something like which would run a statically linked busybox that was provided by the host node and made that enter the cgroups and namespaces and chroot of the container? We already have a short list of features that only work if the host node has some piece of software installed this would be the same yes I ve hit this several times We added oc rsh that provides a helper for itp bash but a more generic static linking solution would be good that works on any container It can t totally depend on busybox because we can t ship support busybox Probably bonus points for allowing this to be more sophisticated down the road and inject other things Is this something we can prioritize? The actual implementation what would it look like? A flag to the kubelet sigh that let s the user choose shell intepreter on host and when the user is running it would just run nsenter but with sh from the host right? cc Anyone up for this? I don t think we d want to override the image s shell it would have to be a fallback iff the shell is missing I have this problem as well see 35584 for how I d like to address it I m looking for additional use cases so your input would be most welcome This isn t a priority for SIG Node but it s the only thing I m attempting to contribute so it s a priority for me I think I disagree with Clayton if we offer this it should be consistent functionality This doesn t have to replace exec but it should work the same in all cases Having a static busybox seems like a good start but it does become one more thing that nodes need to have installed or that we ship with kubernetes On Wed Jan 4 2017 at 1 43 PM Lee Verberne notifications wrote I have this problem as well see 35584 for how I d like to address it I m looking for additional use cases so your input would be most welcome This isn t a priority for SIG Node but it s the only thing I m attempting to contribute so it s a priority for me You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread Let me clarify it is unacceptable for exec to change to magically override the built in shell because that is not backwards compatible and breaks large classes of apps on Kube It s acceptable to make static injection a net new mechanism and might be acceptable to fallback I also don t want a generic shell command that is tied too closely with one particular shell impl we would not be able to ship that for lots of reasons most of which include support So I m 1 to a exec command that uses a shell binary we ship always I didn t mean to imply overriding IMO should stay literal run this command in the user s container If that command doesn t exist fail This proposal was always about adding a net new shell API that was guaranteed to work on any container without the container needing to have any particular files in its image So if you re 1 on that can you explain why? The main reason many people do or is to have a shell and basic tools but as soon as they do that they are beholden to every CVE in those not my app layers It seems best to me to absolve users of the need to do that if possible On Thu Jan 5 2017 at 8 00 AM Clayton Coleman notifications wrote Let me clarify it is unacceptable for exec to change to magically override the built in shell because that is not backwards compatible and breaks large classes of apps on Kube It s acceptable to make static injection a net new mechanism and might be acceptable to fallback I also don t want a generic shell command that is tied too closely with one particular shell impl we would not be able to ship that for lots of reasons most of which include support So I m 1 to a exec command that uses a shell binary we ship always You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread It s a requirement for the runtimes rather than the kubelet and the CRI will need to add support It s the run times that would have to ship a binary Implementation would be pretty straightforward for the current container based runtimes and slightly more difficult for hypervisor cloud runtimes This solution is constrained to a single static binary specified by the Kubernetes developers limiting its usefulness I d much prefer to be able to use an arbitrary container image for debugging but the two aren t mutually exclusive I m ok with a flag that says that makes it opt in But we probably won t support busybox for that so my personal ask is that we try to define what API injected shell is expected to follow before we introduce it has to be really carefully thought through Agree with Lee s points that this won t work with all runtimes and that the real problem isn t just a shell binary it s the other tools that really become important I d probably lean towards a solution like what Lee is proposing than this On Thu Jan 5 2017 at 12 23 PM Lee Verberne notifications wrote It s a requirement for the runtimes rather than the kubelet and the CRI will need to add support It s the run times that would have to ship a binary Implementation would be pretty straightforward for the current container based runtimes and slightly more difficult for hypervisor cloud runtimes This solution is constrained to a single static binary specified by the Kubernetes developers limiting its usefulness I d much prefer to be able to use an arbitrary container image for debugging but the two aren t mutually exclusive You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread I m fine with more robust solutions this was filed as an idea to solve the most immediate piece of the problem but it s just an idea should know by now that 96 3 of my ideas are outright terrible or at the very least are least off target On Thu Jan 5 2017 at 10 16 AM Clayton Coleman notifications wrote I m ok with a flag that says that makes it opt in But we probably won t support busybox for that so my personal ask is that we try to define what API injected shell is expected to follow before we introduce it has to be really carefully thought through Agree with Lee s points that this won t work with all runtimes and that the real problem isn t just a shell binary it s the other tools that really become important I d probably lean towards a solution like what Lee is proposing than this On Thu Jan 5 2017 at 12 23 PM Lee Verberne notifications wrote It s a requirement for the runtimes rather than the kubelet and the CRI will need to add support It s the run times that would have to ship a binary Implementation would be pretty straightforward for the current container based runtimes and slightly more difficult for hypervisor cloud runtimes This solution is constrained to a single static binary specified by the Kubernetes developers limiting its usefulness I d much prefer to be able to use an arbitrary container image for debugging but the two aren t mutually exclusive You are receiving this because you commented Reply to this email directly view it on GitHub 10834 issuecomment 270702682 or mute the thread 0ixjpQVSQ9ekBmGTsBtaPgvhrvks5rPScGgaJpZM4FTpIs You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread Well since I was very skeptical of Lee s changes in terms of scope I can t disagree that this is potentially a much simpler path I think the unfortunate choice is between a limited experience that enables you to at least look at filesystems and potentially download other things into the container for further debugging or a much more complete story that is also very complex Could we somehow bridge the two or find an in between spot? On Thu Jan 5 2017 at 1 21 PM Tim Hockin notifications wrote I m fine with more robust solutions this was filed as an idea to solve the most immediate piece of the problem but it s just an idea should know by now that 96 3 of my ideas are outright terrible or at the very least are least off target On Thu Jan 5 2017 at 10 16 AM Clayton Coleman notifications wrote I m ok with a flag that says that makes it opt in But we probably won t support busybox for that so my personal ask is that we try to define what API injected shell is expected to follow before we introduce it has to be really carefully thought through Agree with Lee s points that this won t work with all runtimes and that the real problem isn t just a shell binary it s the other tools that really become important I d probably lean towards a solution like what Lee is proposing than this On Thu Jan 5 2017 at 12 23 PM Lee Verberne notifications wrote It s a requirement for the runtimes rather than the kubelet and the CRI will need to add support It s the run times that would have to ship a binary Implementation would be pretty straightforward for the current container based runtimes and slightly more difficult for hypervisor cloud runtimes This solution is constrained to a single static binary specified by the Kubernetes developers limiting its usefulness I d much prefer to be able to use an arbitrary container image for debugging but the two aren t mutually exclusive You are receiving this because you commented Reply to this email directly view it on GitHub 10834 issuecomment 270702682 or mute the thread 0ixjpQVSQ9ekBmGTsBtaPgvhrvks5rPScGgaJpZM4FTpIs You are receiving this because you authored the thread Reply to this email directly view it on GitHub 10834 issuecomment 270715713 or mute the thread AFVgVC28WNIKs6o7QomMBiRkFeg1aF95ks5rPTN gaJpZM4FTpIs You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread Agreed In the very best case my proposal will take a long time to fully implement but the complete troubleshooting story is important for Kubernetes overall Ideally we could find a limited solution that makes progress towards a complete solution like to troubleshoot a duplicate pod before being able to troubleshoot a running one The most contentious part of my proposal is mutating the pod spec but that s not a requirement We could instead create a new resource to model this sort of ephemeral troubleshooting container but there where concerns in SIG node about the proliferation of container types and backwards compatibility with tools that introspect the pod I can t think of a way to inject a binary in service of a more complete solution Binary injection seems brittle if done simply and a larger diverging problem to do robustly What if we always injected a toolbox volume into every container and you could make be the equivalent of On Thu Jan 5 2017 at 1 20 PM Lee Verberne notifications wrote Agreed In the very best case my proposal will take a long time to fully implement but the complete troubleshooting story is important for Kubernetes overall Ideally we could find a limited solution that makes progress towards a complete solution like kubectl debug to troubleshoot a duplicate pod before being able to troubleshoot a running one The most contentious part of my proposal is mutating the pod spec but that s not a requirement We could instead create a new resource to model this sort of ephemeral troubleshooting container but there where concerns in SIG node about the proliferation of container types and backwards compatibility with tools that introspect the pod I can t think of a way to inject a binary in service of a more complete solution Binary injection seems brittle if done simply and a larger diverging problem to do robustly You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread Assuming this is implemented using something like container image volumes it has only a few disadvantages 1 It couples troubleshooting tools to pod creation and one of our goals was to get the freshest utilities at debug time and free us from updates due to CVEs unrelated to our code 2 It constrains troubleshooting to a single previously configured toolbox We care about this because we d like to implement a security policy where audit events depend on what image is run 3 It doesn t help troubleshoot a container that s crashlooping as exec is unavailable 4 It means we have construct an awkward container image where bash uses a path of k8s bin k8s etc k8s lib etc That being said this suggestion simplifies many things 2 and 3 can be solved in another fashion 4 detracts from user experience but isn t the end of the world and gcr io google containers could host a canonical version We could relax the requirement for binary version decoupling in 1 if everyone really likes this solution especially if we could figure out a way to asynchronously update the container image volumes in the future I am having a little trouble getting past the awkwardness of squatting on a filesystem path like k8s for every container It feels like meddling in the container s affairs but I could get over it if it s a path forward On Thu Jan 5 2017 at 3 08 PM Lee Verberne notifications wrote Assuming this is implemented using something like container image volumes it has only a few disadvantages It couples troubleshooting tools to pod creation and one of our goals was to get the freshest utilities at debug time and free us from updates due to CVEs unrelated to our code I m not sure CVEs in the debug tools matter much? It constrains troubleshooting to a single previously configured toolbox We care about this because we d like to implement a security policy where audit events depend on what image is run It doesn t help troubleshoot a container that s crashlooping as exec is unavailable How would you like to debug such a container? Once we have shared PID namespaces we could do this as a taglong container rather than a volume It could do everything but access the filesystem of the container under debug Another alternative might be to have a and run that instead of the main command if it has crashed X times in Y seconds e g gdbstub It means we have construct an awkward container image where bash uses a path of k8s bin k8s etc k8s lib etc That being said this suggestion simplifies many things 2 and 3 can be solved in another fashion 4 detracts from user experience but isn t the end of the world and gcr io google containers could host a canonical version We could relax the requirement for binary version decoupling in 1 if everyone really likes this solution especially if we could figure out a way to asynchronously update the container image volumes in the future I am having a little trouble getting past the awkwardness of squatting on a filesystem path like k8s for every container It feels like meddling in the container s affairs but I could get over it if it s a path forward Sorry I would make it random like a UUID which is why I suggested it be exposed as an API field rather than a const Sorry I would make it random like a UUID which is why I suggested it be exposed as an API field rather than a const Oh ok I assumed a configurable default A random path is more compelling It makes things like PATH shared libraries configuration files more difficult We could probably use some sort of exec shim to update PATH and what not but getting bash to find UUID etc bashrc probably isn t possible hmmm One concern I have with auto mounting debug tools into every container is it makes the tools available to potential attackers as well For example if an arbitrary command execution vulnerability gets you to a shell a lot of attack paths become much easier I thought this was the original motivation for the tool being discussed so that we didn t need to build a shell into every container? Tim I think we re still at the brainstorming stage On Fri Jan 6 2017 at 10 34 AM Tim St Clair notifications wrote One concern I have with auto mounting debug tools into every container is it makes the tools available to potential attackers as well For example if an arbitrary command execution vulnerability gets you to a shell a lot of attack paths become much easier I thought this was the original motivation for the tool being discussed so that we didn t need to build a shell into every container? You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread FYI 1A I ve updated the proposal in 35584 with some of the ideas discussed here The latest revision of that proposal favors an approach which I think will give a better user experience It s not exactly what s being requested in this issue but it may serve the same purpose You d have the choice between creating a copy of a pod with arbitrary changes or attaching a new container to the running pod I d be interested to know whether this meets your needs I was just thinking about audit logging and how it relates to some of the proposals discussed here We don t currently have any logging of which commands were run through an exec shell Since exec could be used to stream arbitrary data there s a risk of filling up the audit logs with garbage if we logged all stdin If we introduced something along the lines of then we at least know that there s a shell on the otherside of the link and we could use that to provide a better log of executed commands If we stick with a more generic solution it runs into the same problems as exec In the more generic solution it becomes a matter of cluster policy e g restricting available debug images to the image which logs all commands to some preferred log aggregator I tend to lean towards solving this in an image rather than in the kubelet Do we have a front runner design? On Tue Feb 14 2017 at 1 35 PM Lee Verberne notifications wrote In the more generic solution it becomes a matter of cluster policy e g restricting available debug images to the image which logs all commands to some preferred log aggregator I tend to lean towards solving this in an image rather than in the kubelet You are receiving this because you authored the thread Reply to this email directly view it on GitHub or mute the thread I still think running a inside an existing pod is the most elegant solution but it s contingent on being able to cross mount container filesystems which I ll try to resolve over the next week Maybe a bit unrelated to kubernetes specifically but I just came across It uses some ptrace magic to inject syscalls on host binaries at the right moment It allows the execution of any host binary on any container without the need to mount anything inside the container itself It has solved the problem of debugging containers without a shell for me even though I still have to ssh to the node where the container is running Assigning to Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale remove lifecycle stale Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale remove lifecycle stale  
Secure master node communication Forked from The only remaining communication from the master to the node is the proxying functionality built into the master Moving that functionality out of the master is a reasonable alternative to securing the communication we secure this today via the mechanism mentioned in 3168 which is the unified CA with server cert per node and a client cert from the master right? GKE secures this pathway using ssh tunnels from the master we secure this today via the mechanism mentioned in 3168 which is the unified CA with server cert per node and a client cert from the master right? yes When I tried securing this pathway using ssh tunnels without using a cloud provider it failed see I favor to secure this pathway using fully implemented certificate checks Externalizing the proxying function seems to take a while I d like to see it land first We will also need a secured node API if we embed the pod resource path into the kubelet to create interact with pods using the client This is useful even if we move proxying out of the apiserver How much of this is covered by 14700? What still needs to be done? set up the same flow for the kubelet as the master request authentication authorization attributes builder authorization It added plumbing and tests to make sure the interfaces are exercised but didn t hook up implementations yet The attributes builder implementation would have to be specific to the kubelet since its API looks completely different from the master API This is what we re currently using in OpenShift The authentication authorization interfaces could be satisfied by the same ones as the API server but before we add 10 authn authz related arguments to the kubelet and hook up the same implementations we need to decide whether we anticipate each kubelet managing its own authn authz or delegating to the master In OpenShift we use that checks against the master API using the SubjectAccessReview ResourceAccessReview APIs proposed in Some sort of authentication that prevents access to the kubelet port from anyone but the master is critical for us to implement kubectl exec and kubectl logs There s no way we can open up a port that allows arbitrary code execution without that port being secure TLS would be perfect Is there any chance that might be doable by 1 3? I don t think that it is something that anyone at Google is working on in the 1 3 timeframe Why this issue has a P2 priority? It is a huge security bug Here are a few examples The linked exploit is not possible in secure setups Secure setups bind the handler to localhost and use ssh tunnels from the apiserver to the nodes to implement exec logs attach See I d just like to point out that it is not very well documented that that must be done is the only page that really addresses it and is hard to discover The design document should probably call this important step out as I understood flag instructs only apiserver but it will be still possible to connect to 10250 port please correct me if i m wrong anyway I saw that added authentication authorization interfaces to the kubelet here most probably it will be possible to implement them in k8s v1 4 as for current stable release I suggest to use this simple patch It will require following options kube apiserver kubelet can you tell me which of the current Kubernetes setup tools implement this setup? is this patch in some pending PR? Seems to be the obvious solution the problem that I don t know how to create pull request for previous release but it will be still possible to connect to 10250 port please correct me if i m wrong The idea when using the ssh tunnel is that 10250 is bound to localhost and access to nodes is restricted to cluster administrators TLS client auth sounds like a very reasonable addition I think we would want to limit access to a group and we should be using the cluster CA to provision both the apiserver s client cert and the kubelets serving cert cc sig auth can you tell me which of the current Kubernetes setup tools implement this setup? kube up GCE supports this kube up is implemented per cloud so I can t speak for others kube ups GKE does this by default It s a priority to get this done in kubernetes anywhere as we move towards a production ready release but kubernetes anywhere is alpha for v1 4 I can t speak for coreos It sounds like there is some support for this in Openshift that could be pulled upstream I didn t find any examples where kubelet was forced to listen 127 0 0 1 can you point me on them? You are right it looks like GKE uses a firewall and the ssh tunnel is roundrobin so no localhost I agree that this is not where it should be Moving to 1 4 milestone We are a week after feature freeze I don t think this is going to qualify for v1 4 cc For reference v1 4 freeze merge policy Agree I d like to see this early in 1 5 to allow time to get all parts of the system working with it turned on Masters and metrics are the two clients of the kubelet API I m aware of At box we worked around this by running a daemonset with stunnel an iptables rule to direct off host traffic to 10250 to the stunnel port on 11250 The stunnel does client cert auth so only apiserver certs will be accepted It would be much simpler for us from a management standpoint if this check could move into kubelet itself apologies to but this is the conversation I had in slack with him huff My main thing is that I need restrict only apiservers connecting to kubelet and don t need the finer grained policies currently simple mTLS auth would get me that liggitt yup huff Is the apiserver sending all of the certs that it has flags for? I could put a proxy in the middle for now liggitt API server has kubelet client certificate as far as I know it sends that in kubelet requests kubelet just isn t paying attention to it huff cool it s just kubelet doesn t enforce it due to liggitt yeah though I d probably recommend hooking in x509 authn the same way the api server does huff with common name? liggitt rather than transport level hook up an authenticator that ensures a valid client cert is on the request huff oh okay liggitt can just validate against a ca but that positions us to union x509 with other auth types the interfaces are all plumbed already requiring it at the transport level means no client cert no connection of any kind huff gotcha liggitt the node uses all the same authenticator authorizer interfaces as the api so it shouldn t be hard I just didn t want to commit the kubelet to supporting command line flags we didn t want long term but a single client ca file to get x509 client cert auth doesn t seem onerous cc If this is a bug fix or should require an exception LMK As an issue I believe we should treat this as we would a vulnerability in our ABAC RBAC system because as I understand it the consequences are the same a pod can gain privileges that it should not be able to My understanding is that ABAC is in beta and RBAC is in alpha I agree with that the feature freeze is in place but that should not affect issues That s why I moved the issue into the 1 4 timeframe it appears to bypass shipped functionality It might influence any promotion of ABAC RBAC or other access control to stable in 1 4 I think we should consider the issue and the proposed PR in 1 4 along with alternative workarounds We may decide not to fix it but we should actively decide that if you agree can we move the issue back into 1 4 please? thanks for your support I ve as tiny as it is possible patch for the master branch As an issue I believe we should treat this as we would a vulnerability in our ABAC RBAC system because as I understand it the consequences are the same a pod can gain privileges that it should not be able to My understanding is that ABAC is in beta and RBAC is in alpha This is not new the changes to make this work properly aren t insignificant and behavior change after doing this isn t minor I don t think I d want to ram this through in 1 4 I d prefer to take the time to do it properly in 1 5  
Any panic inside a rest handler causes API server to exit panics are not propagated between goroutines so if a panic occurs inside the goroutine spawned by apiserver resthandler go finishRequest the API server exits because the registered recover handler never intercepts the panic To illustrate this add a call to a validate function like start the api server and attempt to create any pod The API server exits I think a vulnerability to server exit on any panic causing bug probably deserves more than a p2  
Adding coarse grained security policy at service and namespace levels In follow up to the discussion in the I would like to propose an API standard for coarse grained security policy at the service and namespace levels I propose adding a option to the field where services are only accessible within their namespace The service types will be unaffected by the proposed additions In addition I propose adding a field to namespaces with or options Pods within a namespace are only accessible by pods within the same namespace Pods within a namespace are accessible from anywhere in the cluster A service should inherit its policy from the namespace it belongs to therefore if a namespace has a private networkPolicy services within the namespace will have a private type unless otherwise specified Similarly if a namespace has a public networkPolicy services within the namespace will have a ClusterIP type unless otherwise specified These proposed additions to the API will be implemented by the network plugin If the network plugin does not support them the default behavior will not break applications The current default behavior in Kubernetes equates to namespaces with a public networkPolicy and services with a ClusterIP type This will not change for the 1 X releases in order to maintain backward compatibility For 2 0 the default should change to a private namespace networkPolicy and a private service type If a more open policy is desired it must be specified in the spec I m keen to get more general feedback from the community on this as I begin to look into implementing it in the API cc Definitely useful Though namespace is assumed here for a group of pods An extension of group of pods could be expressed using labels Say by using labels Selector and fields Selector we could specify who all are allowed to use the service The advantage is that a service can then define arbitrary set of pods that it is available to rather than just everyone only and within its namespace Same for networkPolicy saying private probably means namespace self Namespace labels labels Everything public likely means namespace NamespaceAll labels labels Everything I agree that label selection semantics are a powerful and useful addition here but I think of that as function that should be added to the API after this first baby step As a tactical first step towards richer policy I d suggest I recognize that clusterIP in this case is a bit of a misnomer and would advocate it being renamed to something like or something appropriate down the road at which point might be renamed to or something appropriate Pods within an namespace are accessible cluster wide and pods within a namespace are only accessible within their own namespace namespaces should only be used if you really want everyone to be able to access the pods within that namespace so Services within an open namespace would always be globally accessible Essentially open namespace no security policy on pods within that namespace Services within a closed namespace would by default be accessible only by pods within the same closed namespace but could poke holes through the namespace boundary if they are declared as or This doesn t prevent adding more fine grained security policy down the road rather it gets the ball rolling in terms of getting network policy defined in the API 1 I d suggest a slight change from the above as suggested by The current definition of is Open Closed only accessible to the namespace closed closed NamespaceIP ClusterIP closed closed NamespaceIP ClusterIP closed closed namespace closed selectors What sort of security policy are you expecting the LoadBalancer to enforce in this case that shouldn t be done at the service level instead? The problem I see with allowing a configuration where for example and are not mutually exclusive on a Service is that NodePort is inherently opening that service outside of the namespace So Doesn t really make sense since Nodes are not part of any namespace Do you allow traffic from the NodePort even though it is outside of the namespace? This case suffers the same problem We could dis allow these configurations but then and mandates the access level so then is really just a type below NodePort and above clusterIP You could limit traffic flows in the way you suggest with fine grained policy For example it might look something like this Thoughts? Security policy on load balancer is things like running an Apache process with mod sso that requires all http requests to services behind it to have a valid cookie from the central domain controller Or denying ingress to a set of services because the source packet has an IP from outside the cluster network That isn t really something the API expresses that s an implementation detail of the load balancer configuration Note that I expect load balancers to cover multiple services in many cases there are certainly times where there is only one but ingress paths and Openshift routes are both designed to program L7 balancers in easy to use ways in bulk Having simple network policies on services is good but it should also be an easy option for an admin to make those choices on top of the cluster based on other criteria in the SIG network we ve been talking about attaching richer policy to Services and I think that would be the way to implement the sort of LB policy you re talking about I think that s the next iteration though and I don t think this proposal closes the door on the sort of richer policy that you want In the next iteration we re going to need something more expressive than just possibly a field where we can specify more fine grained access controls As Rajat says selecting pods based on Namespace is just the simplest way of grouping pods I d like to be able to define this sort of policy on a Service Where Service A could a headless service a normal service or a custom EndpointSet I agree that we should be able to impose cluster wide policy as well this would be a good use case for a generic Policy object that could select all pods I m hesitant to pull the namespace isolation behaviour out into a separate field because as says how do we enforce a NodePort service with Service NetworkPolicy Closed? I think it s better to scope this change in behaviour tightly and not to try to cover LoadBalancer and NodePort services in this iteration As long as we can come up with good justifications My concern is that policy growth leads to rules that are not quite NamespaceIP or ClusterIP for instance I doubt any user wants a service that is purely namespaced scope forever or at creation time is not sure A cluster scoped IP is required to do some forms of inter cluster communication and has the benefit that it is future proof when a user wants to share a service across namespaces If we have both policy and type and I have to change type sometimes and policy other times then we ve created an unclear API We could introduce a policy sub object today and simply say that you can t set policy on nodeports it means the policy and type have subsets that are not allowed I would also note that I would expect to reserve NamespaceIP for a future where the IP was only unique to the namespace However having namespace unique IPs restricts the users future choices I can t switch from NamespaceIP to clusterIP without possibly reallocating my IP which defeats one of the goals of services Service ip mapping is an option but that still requires a unique ip visible to other parts of the space that cannot overlap We make strong statements today about the Kube network and service space being flat and some change is necessary to make that work but for instance I would not want DNS to a service to resolve different in two different namespaces in the cluster unless we had a really compelling use case I see your point regarding API clarity I d note though that in the linked PR these fields are marked experimental to attempt to get something in users hands before trying to build the final policy API As an endgame I tend to agree that it would be clearer to put all policy in a policy attribute rather than having both policy and type NamespaceIP since Policy can describe a superset of the function described here But agreeing on the semantics and syntax for Policy is a big undertaking which is why was keen to try to get something simple in first Also doesn t Type LoadBalancer also create a NodePort? The seem to suggest so If we can t support a particular policy on NodePort service then how can we support it on a LoadBalancer Service? Introducing overlapping IP ranges in Kubernetes seems like a very significant change to the k8s networking contracts is that a use case we need to plan for? I hope not but I ll defer to those with more knowledge of the roadmap on that one I don t know if it s a likely enough scenario to reserve the name for That said happy to switch to some other name if NamespaceIP is too loaded a term I didn t have a concrete use case for a namespace scoped IP but naming wise I think it could be construed ambiguously Regarding fields that only apply to certain types technically we have those already with loadBalancerIP I lean slightly towards introducing a simple policy object with one field than adding a type because it s easier for older clients to ignore a field they don t understand vs a value in an enumeration It s much more likely to break simple clients and simple API consumers On Tue Sep 22 2015 at 2 49 PM Paul Tiplady notifications wrote I see your point regarding API clarity I d note though that in the linked PR these fields are marked experimental to attempt to get something in users hands before trying to build the final policy API As an endgame I tend to agree that it would be clearer to put all policy in a policy attribute rather than having both policy and type NamespaceIP since Policy can describe a superset of the function described here But agreeing on the semantics and syntax for Policy is a big undertaking which is why was keen to try to get something simple in first Also doesn t Type LoadBalancer also create a NodePort? The docs seem to suggest so If we can t support a particular policy on NodePort service then how can we support it on a LoadBalancer Service? Introducing overlapping IP ranges in Kubernetes seems like a very significant change to the k8s networking contracts is that a use case we need to plan for? I hope not but I ll defer to those with more knowledge of the roadmap on that one I don t know if it s a likely enough scenario to reserve the name for That said happy to switch to some other name if NamespaceIP is too loaded a term Reply to this email directly or view it on GitHub The network SIG is working on a proposal so I am closing the existing proposals Please reopen if you think I am wrong  
Impossible to completely eliminate insecure access to apiserver The apiserver listens by default on 127 0 01 8080 in a completely insecure manor Even if the scheduler controller manager proxies kubelets etc are all configured to use the secure port the apiserver still talks to itself on the insecure port This is how things like the admission controllers talk to other parts of the apiserver The problem with having the apiserver talk to itself over the insecure port is that means we cannot block the insecure port from everything else on the host Any user able to log into the machine running the api server can connect to the insecure port and do anything they want Blocking all access to 8080 with iptables means admission controllers and such stop working There may be many solutions but the basic problem is I want to turn off all insecure access to the apiserver suggested some form of privileged kubeconfig the apiserver would use to talk to itself It might also be possible to create a unix domain socket pair which only the apiserver could use to talk to itself in an insecure manor similar to how it is currently using the insecure socket but which everyone else on the host couldn t use The comment for insecure access flag says Do we want to do something with this? You can disable with but it breaks admission controllers At minimum need to plumb client config for the admission controllers to use It would be great to turn this off but it s outside my area of expertise On Wed Sep 9 2015 at 5 08 AM Marek Grabowski notifications wrote The comment for insecure access flag says The port on which to serve unsecured unauthenticated access Default 8080 It is assumed that firewall rules are set up such that this port is not reachable from outside of the cluster and that port 443 on the cluster s public address is proxied to this port This is performed by nginx in the default setup Do we want to do something with this? Reply to this email directly or view it on GitHub We should update the comment that talks about nginx since we removed it quite a while ago UGH Stuff inside the apiserver shouldn t do this Making a client pointing at yourself is a little nuts Have to think about how to fix this cc Would really like to see this fixed for 1 3 Me too Are you guys going to have any bandwidth to work on it? Stuff inside the apiserver shouldn t do this Making a client pointing at yourself is a little nuts I like how you can make sure all the same paths apply I like the ability to give different components different permissions was experimenting with a go library that let you do calls via a channel so that you wouldn t pay network stack overhead but could treat the rest of the handler chain consistently The network stack overhead isn t that bad on modern kernels Even the loopback channels weren t substantially faster although tail latency was much better On Thu Mar 17 2016 at 10 36 PM Jordan Liggitt notifications wrote Stuff inside the apiserver shouldn t do this Making a client pointing at yourself is a little nuts I like how you can make sure all the same paths apply I like the ability to give different components different permissions was experimenting with a go library that let you do calls via a channel so that you wouldn t pay network stack overhead but could treat the rest of the handler chain consistently You are receiving this because you were mentioned Reply to this email directly or view it on GitHub I m surprised that this doesn t get address with higher priority For the sake of a safer world can you please at least require token or basic auth on http endpoints? Currently this behavior by default opens what is effectively a root shell to anyone who can reach 127 0 0 1 That could be containers if the apiserver runs on systems with regular workloads but also local unprivileged users or remote attackers exploiting a like vulnerability Or use DNS rebinding to trigger api calls etc Yes this really needs to be addressed This may be already addressed above but I also see no way to make work with the API Server in SSL only mode unless the API Server s cert is signed by a trusted CA I don t know enough about Go off the top of my head to know if you can customize that like Java without changing code Unless the kubeconfig approach discussed can work with the controller manager and it looks like it does No it doesn t seem to be fixed in v1 3 5 release The problem are admission controllers inside kube apiserver They seem to communicate with api server over insecure connection only 1 I have apiserver run with insecure port 0 2 Every kubernetes process use kubeconfig and master with secure ports specified 3 Yet when I run the apiserver it logs out reflector go 216 k8s io kubernetes plugin pkg admission serviceaccount admission go 119 Failed to list api Secret Get dial tcp 127 0 0 1 0 getsockopt connection refused Always tries insecure bind insecure port While it is mostly fine to use insecure port on machines dedicated for kubernetes only it makes admission controllers unusable at the moment in environments where other processes are running next to the kubernetes processes for security reasons yes my k8s version is v1 3 3 in my log of apiserver k8s io kubernetes plugin pkg admission serviceaccount admission go 119 Failed to list api Secret Get dial tcp 10 10 10 150 0 getsockopt connection refused Am not sure if the approach i came up with in is acceptable or not PTAL Awesome Thanks for this  
NsenterWriter is an arbitrary shell execution path traversal attack waiting to happen EDIT This is not a vulnerability right now but it is a potential one While fixing 16969 we noticed that NsenterWriter has the potential for arbitrary shell execution if a caller doesn t validate their paths We need to ensure that is impossible by invoking on the passed input and then using to verify that the shell we generate cannot be escaped However we may need to go even further Docker 1 10 should have a mount propagation setting which allows shared mount at which point this code should die and be totally removed We should leave the containerized code in experimental and put big security warnings around it until we have that I d also like to figure out how we can have much stronger review policies around places where we execute bash code in the node by requiring it go through a strict review process and having specific helpers I d also like to put some automation in place to ensure we do not execute shell except through whitelisted paths rh platform management goog node A quick review indicated this is the only potential spot for this in the codebase right now but that could change Touchpoint containerized exec 13691 Also if we have to support containerizing the kubelet on pre 1 10 we will need to keep this around do you think that will be a need? I think in this specific case you can just use instead of to avoid needing the shell Issues go stale after 30d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close reopen assign you can t re open an issue PR unless you authored it or you are assigned to it details In response to reopen assign Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details  
make it possible to unmount secrets Many applications have a setup phase where they need credentials and don t afterwards It would be great not to retain the secrets in case of an application vulnerability It might also be nice to do that automatically I think it ought to be an attribute of the container s volume mount Could you instead put them into env vars and unset the env var when you are done? The idea as described is not very exciting to me and tricky to implement I understand the code complexity argument I don t think environment variables are the answer though Coming from traditional Unix I mistrust env vars for storing secrets because they get exposed in proc They also get shown when exporting the pod rc declaration in kubernetes On Thu Jan 28 2016 at 9 58 PM Tobias Florek notifications wrote I understand the code complexity argument I don t think environment variables are the answer though Coming from traditional Unix I mistrust env vars for storing secrets because they get exposed in proc They also get shown when exporting the pod rc declaration in kubernetes Using a secret as an env var is new in kube 1 2 It doesn t show up in your pod resource because it is late bound by the kubelet Anyone who can read your proc pid environ can also read your files or ptrace your process sig node sig storage This is important for security purposes to limit the attack surface of gaining credentials from the filesystem Is there going to be any progress on this? Our current mitigation strategy here is to encrypt the secrets before storing them in Kubernetes and arm the Docker image with that encryption key Then on startup read the encrypted secrets from Kubernetes decrypt them with the local encryption key and then remove that key from the pod filesystem Can you say why env vars are not sufficient? On Thu Oct 19 2017 at 2 20 AM June Rhodes notifications wrote This is important for security purposes to limit the attack surface of gaining credentials from the filesystem Is there going to be any progress on this? Our current mitigation strategy here is to encrypt the secrets before storing them in Kubernetes and arm the Docker image with that encryption key Then on startup read the encrypted secrets from Kubernetes decrypt them with the local encryption key and then remove that key from the pod filesystem You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread My understanding is that environment variables are visible to other processes via so we definitely don t want to be using them if they re visible to other processes Only to other processes in the same PID space or the root PID space who run as root or the same UID You can verify this by trying to open someone else s proc pid environment file It should fail unless you have permissions It would also be visible through which also requires permissions On Oct 19 2017 8 34 AM notifications wrote My understanding is that environment variables are visible to other processes via proc so we definitely don t want to be using them if they re visible to other processes You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread Would the environment variables persist in those places even if the process clears the environment variables internally ? Yes for it is part of the container s specification just as the secret exists in the kube API I don t actually know if proc reflects that sort of change I have never tried but it should be easy to test But env bars are about as private as you are going to get without taking full control and fetching secrets yourself into your own process memory On Oct 19 2017 11 22 AM notifications wrote Would the environment variables persist in those places even if the process clears the environment variables internally ? You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread How is reading a file that exists only on disk during startup to memory not more secure than an environment variable which persists around in ? Its perms on disk are approximately the same as your own proc files so not really more secure but you seemed concerned about files There isn t a way to unmount Secrets I guess I don t know what your hypothetical attack vector is so I can t answer the question properly On Oct 19 2017 12 04 PM notifications wrote How is reading a file that exists on disk while the application is not serving traffic to memory not more secure than an environment variable which persists around in docker inspect? You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread Sure it s permissions are approximately the same as the files But it exists only during startup whereas exists for the lifetime of the container If Kubernetes supported unmounting of secrets then after startup the secret would exist only in the process s memory and nowhere else The key point here is to limit the amount of time the credential is readily available to someone who wants to read it If it s in process memory then an attacker would need to dump the memory of the process to disk find the secret in whatever form it exists and then send it over the network That s a lot more difficult than ing some string or file to a remote server Fair enough though the short answer is that we don t support that for now and as far as I know nobody has proposed it On Thu Oct 19 2017 at 3 58 AM June Rhodes notifications wrote Sure it s permissions are approximately the same as the proc files But it exists only during startup whereas docker inspect exists for the lifetime of the container If Kubernetes supported unmounting of secrets then after startup the secret would exist only in the process s memory and nowhere else The key point here is to limit the amount of time the credential is readily available to someone who wants to read it If it s in process memory then an attacker would need to dump the memory of the process to disk find the secret in whatever form it exists and then send it over the network That s a lot more difficult than curl ing some string or file to a remote server You are receiving this because you commented Reply to this email directly view it on GitHub or mute the thread Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale remove lifecycle rotten This is an important feature request for improved security in Kubernetes containers lifecycle frozen remove lifecycle stale Leaking env vars through debug endpoints is a common attack vector There are a few tools you could use to accomplish this today Perform the operations that need the secrets in an init container Startup as root and drop to an unprivileged user after the initialization Use AppArmor change hat  
Why does GCE salt setup do not use master minion mode? GCE salt setup scripts uses mode Is there a reason GCE scripts does not use master minion salt mode? Security and reliablity 5112 Thanks for the answer Regarding security 1 How do you think anyone who has a large cluster in GCE upgrade things in place when a new vulnerability is exposed? 2 How does one do things like regular key rotation etc without a a master minion salt setup in GCE? Does not that leave GCE cluster more vulnerable in the long run? 3 I am not a security expert so this is probably a naive question Since GCE cluster is already running inside a pretty locked down network is the communication between master slave a major concern I understand in GKE the master is hidden and access is restricted to the GCP project owner But in GCE master is visible So is this a real concern for GCE only setups?  
 create syscall filters for k8s supplied components With docker 1 10 you can create a filter for syscalls that the container is allowed to execute mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code For containers that we provide where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make cc Issues go stale after 30d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
Kubernetes security in master slave salt mode I have a few questions about Kubernetes master slave salt mode How do you think anyone who has a large cluster in GCE upgrade things in place when a new vulnerability is exposed? How does one do things like regular key rotation etc without a a master minion salt setup in GCE? Does not that leave GCE cluster more vulnerable in the long run? I am not a security expert so this is probably a naive question Since GCE cluster is already running inside a pretty locked down network is the communication between master slave a major concern I understand in GKE the master is hidden and access is restricted to the GCP project owner But in GCE master is visible So is this a real concern for GCE only setups? Please re post your question to We are trying to consolidate the channels to which questions for help support are posted so that we can improve our efficiency in responding to your requests and to make it easier for you to find answers to frequently asked questions and how to address common use cases We regularly see messages posted in multiple forums with the full response thread only in one place or worse spread across multiple forums Also the large volume of support issues on github is making it difficult for us to use issues to identify real bugs The Kubernetes team scans stackoverflow on a regular basis and will try to ensure your questions don t go unanswered Before posting a new question please search stackoverflow for answers to similar questions and also familiarize yourself with Again thanks for using Kubernetes The Kubernetes Team I have reposted on stackoverflow  
Ensuring only images are from approved sources are run A customer has asked for a feature which would ensure that only certain binaries are run on the cluster This issue is to get feedback from other users on whether this would be useful and to clarify requirements and to see if some simple implementations would be sufficient Background The threat some users are trying to protect against is that an otherwise authorized user of a kubernetes cluster accidentally or intentionally creates a pod that runs a binary that was built from untrusted possibly malicious source code It might actually cause direct harm or just require costly auditing after the fact Auditing requirements may be dictated by Requirements Questions 1 Is it sufficient to whitelist what image repositories are allowed? For example allow only image names with prefixes or Or is finer control such whitelisting individual tags or tag patterns needed? Or even specific binary content hashes 1 Docker supports specifying images at a specific checksum like Is this allowed and if so how would it work with a tag whitelist? 2 Should the whitelist check be intersected with Docker Content Trust tag signing checks? 3 Does it need to work for all container runtimes or is just Docker enough for now? 4 Is a single cluster wide policy useful at least as a first pass Will we eventually will want policy for each user or per namespace 5 Is a policy that is expresses as a flag useable enough? 6 Is the policy mandatory or would after the fact auditing be sufficient for most users? If it is mandatory how easy does it need to be to override it in an emergency 7 Is image whitelisting useful by itself? Or are restrictions on other parts of a Pod spec needed to meet auditing requirements? Implementation Ideas 1 such as which only allows certain images to run 2 kubelet flag that contains an image prefix whitelist 3 admission controller that checks image strings in pods against a policy objects stored in the APIserver 4 contributed controller that watches for pods with disallowed images and sends alerts somewhere Policy enforcement in the apiserver is easier to change and easier to surface errors but probably more work to get something we all agree on working Policy enforcement or docker can apply to http and file source pods and kubelet or docker are in better position to talk to a private registry if more complex interaction is later needed A docker based solution could be applicable to all users including non kubernetes users A kubelet or apiserver solution would apply to multiple container runtimes Only the apiserver can easily have a per user policy since only it is aware what user is creating the object Only apiserver or kubelet can have a per namespace policy since docker does not know or care what a namespace is Related Technology Docker Notary and Docker Content Trust checks the integrity of the image and only allows images with signed tags to run Docker Content Trust uses Docker Notary is a general solution to compute checksums of collections of files and publish signed checksums of those files Docker Notary could also sign things not in images like configMaps for example The docker client is responsible for setting whether trust should be enabled and what notary server to use on each call to docker Docker content trust only applies to image tags A user who asks for a specific content hash e g always succeeds So it isn t clear how to use Docker Content Trust by itself as a solution to prevent untrusted images from being run rkt Trust works similarly to Docker Content Trust It controls which registries are trusted Integrity Measurement Architecture The Integrity Measurement Architecture provides a way for parties to remotely verify the integrity of programs running on a machine It appears to be a quite complex system It requires a kernel that supports it and I imagine it is complex to debug I am not considering it at this time Openshift Builds and Imagestreams Openshift provides an image build system and a private registry for each cluster or each tenant of a cluster Having image builds happen on the same cluster as images run may be convenient in some cases but do we need to mandate this to use this feature? Some users have their own build system already setup Per cluster registries Kubernetes has an open issue to provide a per cluster registry as an add on On GKE clusters have access to a Project wide Image registry However some customers may have existing private registries We should not require them to change that to take advantage of this feature sig auth l We had been planning to implement this soon I ll follow up here with our approach Re implementation why not mixing docker content trust with existing kubernetes authz approach That is apiservers will perform the image tag validation while docker content trust will verify that the image is from a trusted authority This can be achieved by forcing content validation on all image pulls Then extend kubernetes authz approach in apiservers with image whitelisting We would benefit from and use this functionality However it would be valuable only if the whitelisting applies to all of the containers that are part of the dependency chain for a given container E g if we have an organizational policy that we build all base containers in house instead of using anything from docker hub we would need to ensure that when we run the container that the FROM for that container is an allowed container and that that container s FROM line is allowable and so on until there is no FROM line Here s the approach we were going to take in OpenShift allowing the following sets of rules 1 Only use images on the platform that have a set of known defined by a specific policy 2 Only use images on the platform from a whitelist blacklist of known registries 3 Only allow images to be used that come from the internal registry 4 Only allow images to be used that have a certain label or annotation that annotation could be anything but typical use cases would be an admin scanning process that flags images that have been scanned Until they re scanned the platform will refuse to create pods that use those images 5 Require images to have an attached detached signature before being used 6 Limit images that are older than a certain date from being run 7 Perform a call to a remote system prior to allowing an image to be run We re still trying to gather additional use cases but most of them here are at the platform level not at the node level I think it s assumed that if node level verification can be performed it should but that most administrative policy would happen above that level do you have an automated docker image build pipeline which has they keys to push to your trusted repo? If so could that automated process be the thing responsible for ensuring that the FROM of the Dockerfile is an allowed base layer? I m trying to separate this problem into two parts and Operational teams may delegate access to build images to users but still control the trusted bases We hear that a lot So a trusted image is only trusted because it comes from a trusted base On Tue Mar 22 2016 at 8 15 PM Eric Tune notifications wrote I m trying to separate this problem into two parts and You are receiving this because you commented Reply to this email directly or view it on GitHub Could you add an appropriate priority label? Are you planning to have an object that is stored in the apiserver? You said in item 7 above that you would Perform a call to a remote system prior to allowing an image to be run Are you guys envisioning an admission control plugin in apiserver that calls out to an service? Or a kubelet or docker call to an image approval system? Or some other mechanism? For item 7 what info goes to the remote system? Image name? What else? Do you have a use case for overriding the policy in an emergency? Would an admin role be able to bypass these checks? If so how is username or role propagated to the point of enforcement? belatedly we do a lot of automated builds with Jenkins though the security story with Jenkins isn t great The gist of your suggestion is that it d definitely be easier on Kubernetes if it needed only to be a link in a trust chain i e trust a registry that trusts a build pipeline etc and there s definitely validity there from the k8s engineering perspective In our case establishing trust throughout the entire build process is difficult whereas Kubernetes represents a place where it can be done once and if done right be done effectively If I were to have built a trusted pipeline it would be largely wasted effort since Kubernetes could still be directed to run an image originating from elsewhere Knowing that I would heretofore have no reason to embark on such work Whereas if Kubernetes were the hypothetically only place I could build trust it would be effective If Kubernetes will only run images that pass some authorizer then it doesn t matter where the candidate images come from letting me secure my registry and build pipeline as an exercise in defense in depth tl dr Our pipeline isn t trusted and I appreciate the benefits of Kubernetes being just a link in a trust chain but I think implementing the security more directly in K8S would be effective and powerful I agree this is why we re doing similar in OpenShift Admission on running pods has an advantage in that it becomes the final arbiter Unfortunately it requires a tight coupling today in order to accomplish that Summarizing conversation so far in my own words Want a whitelist blacklist of known registries Want rkt trust docker content trust for any registries used Does not provide user namespace granularity so can t be only solution Important that images have trusted base Transitive check needed While you could have a trusted build process that is the only thing allowed to build images and which ensure the trusted base property this isn t openshift and other users on this thread are thinking about the problem For example some users want to delegate building to devs at least in some cases Need a way to scan and label images after build time Need a way to refuse to run images that are not scanned Openshift plans support making a call to a remote system prior to allowing an image to be run Questions I still have what attributes would go to a remote system? User namespace image name what else? some users would like an integrated solution Should we implement the policy as objects within kubernetes what happens if an image s status changes after it hits a kubelet? After RC using it is created? What are expected failure modes and surfacing? need Acting As support to propagate user name from controllers to pod creation so username can be checked when creating pod? Concrete proposal at 27129 Proposal is checked in and webhook API is lgtm ed is working on the implementation of the webhook I have some spare cycles so let me know if there s anything I can do to nudge things along And OpenShift plans on implementing the remote hook for this at some point to point to our new image policy engine Probably not until the 1 5 or 1 6 timeframes though On Wed Aug 10 2016 at 4 30 PM Q Lee notifications wrote Proposal is checked in and webhook API is lgtm ed is working on the implementation of the webhook I have some spare cycles so let me know if there s anything I can do to nudge things along You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread this is done and can be closed right? Yes this is done and went out alpha in 1 4 Yes this is out as an alpha feature Is there documentation on this feature someone can point us to ? I believe could be what you re looking for? We have been exploring enabling Docker Image Signing in our clusters and we looked into the Image Provenance Architecture in the process We believe that there is a use case in which we can augment the architechture for two additional features The two additional we have identified are Image Tag Rewriting ImagePullSecrets propagation Image Tag Rewriting This feature comes about from an issue highlighted in the architecture documents as follows The Backend needs to be able to resolve tags to IDs If the Backend resolves tags to IDs there is some risk that the tag to ID mapping will be modified after approval by the Backend but before Kubelet pulls the image We will not address this race condition at this time By performing image rewriting for example the image would be rewritten to use the image ID instead This allows the docker daemon to enforce the image being pulled from the notary server This concern stems from the scenario where an attacker has compromised a registry server In this case we believe it is feasible for the attacker to determine a request from a webhook verifier from a kubelet pulling the image In that case the malicious attacker is able to always serve the correct image to the verifier and the tampered image to the kubelet To do this the webhook would have to return a list of image names to rewrite i e it would have to return a list for and a list for This could potentially be added to the struct in ImagePullSecrets Propagation This feature comes about from the possibility that the image that the user wants to use is part of a private repository Most commonly auth details are stored in We think that it would be helpful for the Webhook to have access to such information A preliminary implementation idea would be to initialize the admission controller with an retrieve the and augment the sent parameters in the struct in W R T Image Tag Rewriting I know this is being discussed but I m not sure where it s written down A decision needs to be made about where the translation point happens and for consistency reasons that probably needs to be before the image name reaches the kubelet for running Please see for a discussion of ImagePullSecret propagation There are a lot of concerns with it and I ve convinced myself that something like an oauth flow can solve the problem I d love to hear different ideas if you have them Thanks for the feedback By the way do you have a image policy webhook that I can take a look at It would be great if there is something that I can play around with W R T Image Tag Rewriting I know this is being discussed but I m not sure where it s written down A decision needs to be made about where the translation point happens and for consistency reasons that probably needs to be before the image name reaches the kubelet for running Agreed It would be nice if somehow I can view jump in on this discussion I ve played around with admission controller where I did rewriting at the admission controller level which seems like an OK idea Please see 31524 for a discussion of ImagePullSecret propagation There are a lot of concerns with it and I ve convinced myself that something like an oauth flow can solve the problem I d love to hear different ideas if you have them I have do delve a bit deeper into the discussion but it seems like there s a differentiation between use cases of ImagePullSecrets VS Non ImagePullSecrets and External Party VS Extension of the Cluster EDIT Reading through the ImagePullSecrets thread I guessed I only looked at the case I think oauth is a nice framework to do things I will have to look into closer details The registries and notary servers already support the oauth framework to perform the delegation which is nice Just for sharing In playing around with this I had a very simplistic view on this in terms of just extracting the secrets and passing it along in the same way that the kubelet would consume the And just passing that along in the I have implemented the image rewriting on the admission controller Do you think we should have this be merged? Or should we discuss this more? This addresses the problem in the documentation The Backend needs to be able to resolve tags to IDs If the Backend resolves tags to IDs there is some risk that the tag to ID mapping will be modified after approval by the Backend but before Kubelet pulls the image We will not address this race condition at this time However this is not just a race since it is possible for a malicious actor to probably fingerprint whether it is the docker agent or webhook that is querying for an image I sketched this out a couple of weeks ago with a slightly different approach I was concerned that anything we do here would be usurped by which intends to rework admission controllers and explicitly avoid mutating webhook backends for the first version However this is not just a race since it is possible for a malicious actor to probably fingerprint whether it is the docker agent or webhook that is querying for an image This is true but I think you re just highlighting that any admission controller that can mutate the request needs to be as trusted and as secured as any of the rest of the control plane Mutating admission web hooks seems likely in 1 8 if you can wait that long it s one approach On Tue Jun 6 2017 at 8 11 AM Evan Cordell notifications wrote I sketched this out a couple of weeks ago with a slightly different approach master ecordell imagereviewwebhook digest I was concerned that anything we do here would be usurped by kubernetes community 132 which intends to rework admission controllers and explicitly avoid mutating webhook backends for the first version However this is not just a race since it is possible for a malicious actor to probably fingerprint whether it is the docker agent or webhook that is querying for an image This is true but I think you re just highlighting that any admission controller that can mutate the request needs to be as trusted and as secured as any of the rest of the control plane You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread I sketched this out a couple of weeks ago with a slightly different approach master ecordell imagereviewwebhook digest I like your dictionary implementation more actually Looks a lot neater I wrote mine in consideration that there may be more things in the container spec in the future that we may require rewriting but seems like there isn t a usecase in sight at the moment I was concerned that anything we do here would be usurped by kubernetes community 132 which intends to rework admission controllers and explicitly avoid mutating webhook backends for the first version Given the new design would the image policy admission controller be an or a ? Also should we update the document to express the intention of the to also perform mutation since this is something that we want? For the current build i suppose it could be possible to add a flag in the config file to the webhook to perform rewriting or disallow the rewriting so perhaps it may be a moot point to introduce the feature now and take it out again until 1 8 unless there will be support of the admission controllers It would be nice to be able to get this in a little sooner than that if possible Understand that there may be a need to rewrite the code redesign parts of it later Is there anything we can do to help on the work on mutating webhook admission webhooks? Would definitely like to see it in 1 8 Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale  
I ve found 3 vulnerabilities of kubernetes on NVD did they solved? There follows those three vulnerabilities of kebernetes 1 CVE 2016 1906 Summary The API server in Kubernetes might allow remote attackers to gain privileges by editing a build configuration to use a restricted strategy Published 2 3 2016 1 59 09 PM CVSS Severity v3 9 8 CRITICAL v2 10 0 HIGH 2 CVE 2016 1905 Summary The API server in Kubernetes does not properly check admission control which allows remote authenticated users to access additional resources via a crafted patched object Published 2 3 2016 1 59 08 PM CVSS Severity v3 7 7 HIGH v2 4 0 MEDIUM 3 CVE 2015 5305 Summary Directory traversal vulnerability in Kubernetes as used in Red Hat OpenShift Enterprise 3 0 allows attackers to write to arbitrary files via a crafted object type name which is not properly handled before passing it to etcd Published 11 6 2015 1 59 00 PM CVSS Severity v2 6 4 MEDIUM Have those vulnerabilities been solved or how to avoid those risks? THS The first one sounds RH specific sig auth or might be able to comment on the second might be able to comment on the third All three have been resolved CVE 2016 1906 is OpenShift specific fixed in CVE 2016 1905 was fixed in CVE 2015 5305 was fixed in Thanks a lot  
Client certificate auth is using the subject CN from the intermediate CA cert not from the end entity cert Thanks for filing an issue Before hitting the button please answer these questions I tried slack someone else ran in to my issue as well and just used separate CAs intermediate certificate FEATURE REQUEST If this is a BUG REPORT please Fill in as much of the template below as you can If you leave out information we can t help you as well If this is a FEATURE REQUEST please Describe in detail the feature behavior change you d like to see In both cases be ready for followup questions and please respond in a timely manner If we can t reproduce a bug or think a feature already exists we might close your issue If we re wrong PLEASE feel free to reopen it and explain why Client Version version Info Major Minor GitVersion GitCommit GitTreeState BuildDate GoVersion Compiler Platform Server Version version Info Major Minor GitVersion GitCommit GitTreeState BuildDate GoVersion Compiler Platform aws NAME CoreOS ID coreos VERSION 1122 2 0 VERSION ID 1122 2 0 BUILD ID 2016 09 06 1449 PRETTY NAME ANSI COLOR HOME URL BUG REPORT URL Linux ip 10 0 0 50 ec2 internal 4 7 0 coreos 1 SMP Tue Sep 6 14 39 20 UTC 2016 x86 64 Intel Xeon CPU E5 2686 v4 2 30GHz GenuineIntel GNU Linux kube aws Manually generated ssl certs I m using client cert ABAC auth and have audit logging enabled Clients using SSL certs for auth show the subject CN of their intermediate cert not their entity cert I set the client cert to a certificate chain Entity Cert Intermediate Cert Root Cert ca pem on all machines is set to the Root Cert I expect the client to use the Subject CN of the entity cert The key I m using is for the entity and the connection is secure My audit logs just show the CN for the intermediate CA Create a root CA cert Use it to sign an intermediate CA cert Use root cert as CA use certificate chain bundle as client cert use correct client key I m seeing this in audit logs when I use the chain 2016 10 10T23 04 07 466636575Z AUDIT id ip method user as namespace uri When I set the ca pem to my sub ca key and don t use my bundled certs I see 2016 10 11T00 40 33 367066951Z AUDIT id ip method user as namespace uri Please use CVE 2016 7075 for this issue This doesn t affect setups without intermediate certificates am I right? In that case this doesn t affect kubernetes installed with kube up sh on AWS for example Am I right? Does it make sense maybe to list installation methods and say if they are affected or not? the bug prevents intermediate signed client certs from working correctly but is also a vulnerability for clusters using client certificate authentication the fix is available in master and  
Dynamic provisioning with Secrets can leak any secret on the system When a dynamic provisioner needs some credentials to a remote storage appliance we use a reference to a Secret instance in We also agreed that the StorageClass does not need to exists at the point when a provisioned volume is being deleted therefore we must pass all information in annotations of the provisioned PV So the PV typically has URL of the remote appliance pointer to secrets Now anyone who can write PV annotations can steal any secret on the system by forging a PV with URL to a fake storage appliance and pointer to a super secret Secret When a volume plugin tries to delete the PV it will read the referenced secret and pass it to the fake storage appliance attacker has the secret While it s true that the same people who can edit a StorageClass are likely the same people who can annotate a PV it is MUCH less obvious that annotating a PV is such a security risk to the system No cluster admin is going to accidentally give a user permission to manage StorageClasses but a cluster admin could certainly give a user the ability to annotate a PV without realizing how dangerous it is for the time being assume that StorageClass instance available at all times when a deleter might need it We do not need to pass any references via annotations If admin deletes a StorageClass all PVs that should be deleted but can t will enter Failed state with some reasonable message I e admin can find these easily and do something with them we should move all these provisioners and deleters to pods in specific namespace where we can give them necessary credentials via usual Secret in the same namespace sig storage I will start filling PRs for affected volume plugins next week This is another case of referential integrity causing problems To be clear on the problem statement you talk about annotations because that s how the feature is currently implemented Once the annotation for class becomes a real field is this a problem? It seems like the root of the problem is allowing users to write to PVs in the first place I know upstream ACLing is still not fully baked but that seems like the solution no? As an alternative design we have finalizers now You could set a finalizer on the class for each PV you provision This would prevent the class from being deleted until the PVs were gone I don t know that this will scale the say we want and I am pretty sure we do not want an RMW refcount in there What happens now is that on provision we copy the secret namespace name as an annotation onto the The reason this was done was so that on de provision we wouldn t need the to actually do the de provision This is suggest we stop copying that information onto the If the or the gets deleted you cannot de provision the This is suggest we stop copying that information onto the PV If the StorageClass or the Secret gets deleted you cannot de provision the PV Although removing the reference to the secret from the PV annotation doesn t fix the main vulnerability instead of it is Since the existing solution is really bad and this slightly improves the situation FWIW I m ok with this change s idea of having a finalizer on the StorageClass that prevents deletion until the provisioned PV would be a good way to prevent the drawback of not having the secret info in the PV object That said I ll defer final decision to I m wary of abusing finalizers just yet It s not clear that this is really going to be what we want I think I am OK to trigger an error in case of StorageClass going away OTOH writing to a PV s annotations is a pretty high level of privilege for any ideas On Tue Oct 18 2016 at 6 43 PM Saad Ali notifications wrote This is suggest we stop copying that information onto the PV If the StorageClass or the Secret gets deleted you cannot de provision the PV Although removing the reference to the secret from the PV annotation doesn t fix the main vulnerability instead of it is Since the existing solution is really bad and this slightly improves the situation FWIW I m ok with this change idea of having a finalizer on the StorageClass that prevents deletion until the provisioned PV would be a good way to prevent the drawback of not having the secret info in the PV object That said I ll defer final decision to You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread I restate my position that I m fine with the system being unable to de provision a PV if the admin deleted the storage class We can work in the future to try to keep the storage class from getting deleted But we also have to work to keep the secret from getting deleted before any of that actually matters To further scope down the targeting ability makes the 3 provisioners that look up secrets require a secret type matching the volume plugin name That at least requires the creator of the secret to have intended the secret to be used with that volume type  
Tracking Kubernetes Product Security Response Plan This is a tracking issue for creating documenting and exercising a Kubernetes Product Security Response Plan The overall goal is to ensure that when a security vulnerability becomes known the entire Kubernetes community has processes infrastructure and communication channels to make an organized and responsible response for users The document describing the initial implementation of this Special thanks to the many folks who helped to review and make additions to this document Kees Cook Jess Frazelle Matthew Garrett Jordan Liggitt Kurt Seifried Davanum Srinivas and many many more Next steps I would like to have completed by KubeCon Cleanup the response doc and try and put the relevant parts into the dev guide and kubernetes home page Create a private github repo and a private CI system to handle private disclosures Create a vendor survey for people to sign up for embargoed disclosures Create a process for adding removing people from the product security team ack will draft something cc mohr Got caught up with KubeCon madness Targeting getting this done this week Any progress on the requisite systems ? I don t have the permissions and pinged a few people I ll reping sorry for the delay On Nov 15 2016 10 31 notifications wrote Got caught up with KubeCon madness Targeting getting this done this week Any progress on the requisite systems You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread no worries I think we were all overly optimistic about the ability to get stuff done before KubeCon Is there something I can help with externally? or might be able to pitch in ping mohr I was told to get your approval First docs PR is up did you get a chance to draft anything re dev docs PR is up dev docs PR moved to community repo Just to update working on test infra things here what is the status of your test infra? I am updating the process document now and adding all of the release managers to the security list yeah i gotta force someone on eng prod to review and probably sit with them to test will do next tomorrow if not next week Got it I have made major improvements to the security process doc Hoping to get this merged this week So also along with the CI stuff if we want the bots on that repo we need to add seats for them we probably just need the k8s ci bot to start and we can merge manually if all tests pass and skip the merge bot And then we might need a couple seats for engprod to help as well On Sun Jan 29 2017 at 1 40 PM Brandon Philips notifications wrote Got it I have made major improvements to the security process doc kubernetes community 126 Hoping to get this merged this week You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread The security release process doc is merged Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale The PST is drafting something now remove lifecycle stale Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
Docker Vulnerability CVE 2016 9962 Thanks for filing an issue Before hitting the button please answer these questions No Docker CVE Bug Potential Problem Report Please see Docker CVE 2016 9962 related to insecure opening of file descriptor allows privilege escalation Which version of docker are we working with? Do we have a plan to upgrade to a non compromised version? Originally referenced in cc More info CVE 2016 9962 has been fixed in docker Docker v1 12 has been validated in 28698 and latest validated version is v1 12 3 see Maybe just need validation for v1 12 6 My experience with 1 12 6 going from 1 11 2 is that it ended up breaking networking in my cluster I upgraded a pre existing cluster that was already running on 1 11 2 on Ubuntu 16 04 I don t know 100 if I upgraded Docker incorrectly as in downgrading I had to delete the var lib docker network directory for it to start up properly The issue with 1 12 6 was that after a few days from the upgrade that was working fine suddenly the networking broke with no ability to communicate outside the cluster only within it It is using Kubernetes 1 5 2 and Weave as the CNI you weren t there in today s sig node meeting so this issue was skipped I discussed with last week and here are our thoughts First the CVE is not good but in kubernetes most pods have access to the apiserver and can do more damage if they are malicious Normal users who don t enforce extra security measures should not worry too much about this CVE For users who do care about this specific CVE The kubernetes master branch tests docker 1 2 6 on ubuntu in the node e2e test suites Users can probably switch to 1 12 6 For 1 5 we tested 1 12 3 I believe 1 12 6 should also be compatible We can consider bumping the docker version to 1 12 6 in the 1 5 node e2e branch For the users who care about this CVE they can switch to 1 12 6 For 1 4 we simply don t support 1 12 and it d be too much work to make it compatible Users are recommended to upgrade to kubernetes 1 5 instead These are just general guidelines Users should first check with their k8s cloud os vendors and see if the docker they use have been patched and which version of docker is recommended For GCI and the debian based CVM images the GCI GKE team are working on patching them Thoughts? This not only would help security but also reliability as the current version has deadlock issues and in our case we have to restart the docker services docker on nodes at least 2 3 times a week making all containers restart Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale close  
Napkin design bulk namespace access control and or RBAC Note mostly a brain dump As we look at some of the requirements we have around RBAC at scale in Kube three in particular come up often 1 organizational multi tenancy where a large fraction of the cluster is a single organization entity that desires subtenant access 2 sparse multi tenancy where a user has access to a few scopes 3 scoped integrations where an integration has access to a percentage of namespaces resources or resources within namespaces such as an ingress controller that is only allowed to see ingresses and secrets that a user has decided to expose to the end user Our current RBAC scopes are cluster namespace and name The RBAC rules for those scopes map directly to the access pattern OpenShift identified early on that we would need to support a sparse multi tenant model so we introduced a separate resource which is a virtual resource on top of namespace Listing a project queries to an index maintained by a reflector on namespace roles rolebindings We support a limited form of watch over that index although it has some gaps A user can perform a cluster scoped call and see only the namespaces they have the permission on In order to satisfy the 3 use cases above we ve been considering a few incremental stepping stones 1 Support label selectors on RBAC roles to offer subset access grouping namespaces by label selector by organization exposing a limited subset of secrets to an integration for a particular use case 2 Investigate indices and optimizations for converting the RBAC state into bounding set filters on LIST and WATCH the primary challenge is supporting snapshot access LIST without consistent WATCH is useless today WATCH does not terminate when ACL is revoked in practice access to RBAC changes slowly if we can make LIST WATCH correct then we can simply terminate watches when their bounding set changes delta informers already handle bounding set changes on resync we are allowed a grace period to react to RBAC for other reasons so it s reasonable to coalesce role changes over small windows sparse bit indices are very reasonable for the likely distribution of RBAC roles many of the approaches in the literature are trivial to apply to sets this small 3 Investigate continued optimizations for filtered LISTs in storage and etcd at some point we will switch from eager decoding of stored objects to a lazy model that defers full object decoding which leaves room for selective schema decoding now that we have unified the underlying common api group we can do selective decoding of parts of object meta for both JSON and proto JSON by using something like etcd3 allows us to do MVCC reads on uncompacted history that should make it possible to make multiple list calls in sequence for disconnected spans we ve discussed materializing label indices into etcd etcd3 does not have a way to retrieve multiple ranges in one request that potentially would help optimize high selectivity cross namespace calls Subdividing secret access is becoming increasingly important we currently can t enable ingress on multi tenant clusters without significant concerns about escalation The more integrations the cluster has the more the coarse model for ACL runs into limitations has anyone discussed requested the ability to perform multiple range retrievals in one request in etcd3? While we can dispatch multiple requests for low N fetches being able to say in a single call would make some of the scenarios described above more feasible for higher N If my understanding is correct it is already possible today in etcd3 You can have N range requests in one Txn to fetch N ranges We need to rework the api storage API to enable txn though Then we can unlock more powerful features cc thanks I misinterpreted the docs on TxnRequest Would the impacts of scale well with N range requests? Or do you foresee other limitations? As far as storage api I think we d need to carefully understand how the interplay between authz and API would work I would prefer a model where the authz layer builds a filter query that can be efficiently implemented such as a field selector for namespace IN X The piece makes me think of discussions we ve had about limiting per Kubelet access to API objects to only what that Kubelet needs This might not be as easy as a label selector RBAC rule but we should open up that discussion as well As far as storage api I think we d need to carefully understand how the interplay between authz and API would work I would prefer a model where the authz layer builds a filter query that can be efficiently implemented such as a field selector for namespace IN X So long as the abstraction for filtering is external to the client constructs I have to admit I m left feeling uneasy at the thought of constructing multi op filters on the fly and ensuring traceability which tbh were not to good at today We re going to need a audit trail I need some more clues to understand the concept you mentioned in the opening remark here What do you mean by ? Do you mean that GET returns the right namespaces? That would be an odd thing for a GET of v1 projects to return Or you do mean the get returns things found in the right namespaces? How does this connect to the problem I want to solve which is to control what happens in API calls to get list watch etc normal things ? How does this connect to what is returned from a DNS lookup? I looked at the README md of but do not understand it I did not see any parameters to the project creation nor a way to specify any visibility rules for a project I do not see examples of how to use a project I do not understand the remark Do I need OpenShift in order to follow the recipe there or will plain Kubernetes do? I suppose that in the enumeration of 3 items in the opening remark here you meant to end item 3 with a reference to the ingress controller rather than the end user I suppose that in the second enumeration of 3 issues issue 2 was meant to refer to burstiness rather than bustiness Subdividing secret access is becoming increasingly important we currently can t enable ingress on multi tenant clusters without significant concerns about escalation Can you explain the escalation vulnerability? I assume that a Secret in the field of the Ingress spec has to be in the same namespace as the Ingress object So I don t see how it would be possible for me to have my Ingress read Secrets in someone else s namespace I guess the vulnerability must be something else You have to give the ingress controller the ability to read secrets Which gives it the ability to do anything in the namespace you granted There s no way for an etcd operator to utilize secrets without also being given effective root access Two questions about what you just wrote 1 If there s one Ingress controller per cluster I guess it must use a service account that can read all secrets in the cluster But above you said Can you clarify? How is it granted permission for just one namespace? 2 If there is one Ingress controller per namespace and the Ingress controller uses the namespace s service account then I can see that it would only have access to one namespace s secrets You wrote But can t you configure it to use a different service account that only has permission to read secrets? Which gives it the ability to do anything in the namespace you granted If the user grants access to any secret it can do anything that any service account can do Anyone wanting to run an ingress controller across multiple namespaces would have to manually grant the ingress controller permission to see its secrets which would be painful For an operator any operator that creates secrets probably needs to read them since reading reveals the secret an operator that wants to create and reference secrets usually asks for Of course if the operator can create pods then the operator can simply run a pod that references the secret and exfiltrates the data Has there been any more thinking or work on the stuff described in your first message in this issue? BTW just for clarification IIUC the feature requests here are add label selectors to Role and ClusterRole so that instead of only being able to select a single object or all objects you can specify all objects matching the selector This allows you to add objects to the set permitted by an RBAC rule without adding or updating RBAC rules you just create an object with the proper label Because that label gives someone something permission to do stuff there s no security risk if we simply anyone to set that label on the objects they create they can only make themselves less secure not make someone else less secure fix native well integrated support for be able to do LIST WATCH on Is that right? Probably allow users to bulk retrieve items by namespace instead of making multiple namespace calls On Sat Aug 19 2017 at 7 08 PM David Oppenheimer notifications wrote Has there been any more thinking or work on the stuff described in your first message in this issue? BTW just for clarification IIUC the feature requests here are add label selectors to Role and ClusterRole so that instead of only being able to select a single object or all objects you can specify all objects matching the selector This allows you to add objects to the set permitted by an RBAC rule without adding or updating RBAC rules you just create an object with the proper label Because that label gives someone something permission to do stuff there s no security risk if we simply anyone to set that label on the objects they create they can only make themselves less secure not make someone else less secure fix native well integrated support for be able to do LIST WATCH on Is that right? You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
Hyperkube image contains vulnerable rootfs components Currently the hyperkube image ships with binary components and libraries with outstanding CVE s See quay io s scan of a hyperkube image based on the upstream build scripts Clearly a vulnerability in a rootfs component does not quite equate to a vulnerability in a hyperkube binary running inside this rootfs The Hyperkube component would need to shell out to the binary using attacker controlled input The problem is that this makes it difficult to assess if any of these vulnerabilities should be taken seriously and it also makes it difficult to fix if the current debian base image doesn t have a patched version available upon rebuild Long term i m not sure the best way to fix this but parring down the amount of the components in the rootfs where possible would probably fix a few of these warnings cc Which kubernetes release does this hyperkube image correspond to? Looks like hyperkube is based on debian jessie 40248 is a related issue That corresponds to a recently built 1 5 2 release post Sorry I didn t find 40248 in my searches should I close this in favor of that? How is hyperkube used? Is it feasible to rebase it off scratch or does it require a shell? Any other external dependencies? the Dockerfile currently additional dependencies Not sure how many of these are crittical though Oh yeah I guess this is going to be a union of the dependencies of all the components We may need to special case this image since it requires so many dependencies Also a couple of those packages don t seem to have alpine versions It probably makes sense to keep it on Debian and just make sure we keep it up to date Do any of our E2E s use hyperkube? cc based on related 40248 A couple notes Very few of those CVEs have upstream fixes Not all of those findings are valid only applies to iOS only applies to sshd Remaining issues are unfortunate but mostly look to triaged and marked unimportant by debian but I haven t looked at all of them I don t think changing the base makes sense at this time Not all packages are available on alpine and the CVEs I looked at were not fixed on Ubuntu either sig release Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale The hyperkube image is now based on debian stretch with a much more limited set of dependencies Do we think this issue is still relevant? Seems like there is some extra cruft lying around in some of the layers It still deflates to 600MB Yes as I noted in the hyperkube image still has quite a few dependencies due to the way hyperkube is used Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
End 2 end encrypted kubectl exec using SSH as protocol The current kubectl exec implementation is using the transport encryption from the kubectl to the apiserver and the transport encryption from the apiserver to the kubelet to tunnel the terminal data The terminal data itself is not protected otherwise i e the apiserver can read everything in plain text This is a potential attack surface as taking over the apiserver allows to watch execs and or to even initiate them to attack any pod and compromise it The basic idea is use end 2 end encrypted connections from kubectl to the pods for kubectl exec using SSH as the binary protocol by letting the kubelet spawn ad hoc SSH servers on each connection add an ssh subcommand to kubectl like The SSH public key will be provided somehow to the kubelet One approach would be to pass them as an annotation of a pod But as the apiserver by assumption cannot be trusted also the annotation cannot be trusted This means that measures are necessary to protect against the apiserver playing man in the middle by faking the public keys in the pod s annotation So probably the kubectl needs some kind of identity check that it is talking to the kubelet and no man in the middle If the apiserver is taken over you can root the cluster But it s even easier to just do this yourself start a session with ssh over exec What does this add? Ie if you own the apiserver it s literal game over for the cluster until we introduce the concept of Ie if you own the apiserver it s literal game over for the cluster until we introduce the concept of This is the point We security wise build a single point of failure If you exploit some security vulnerability of the master the cluster is rooted as you say anything can be executed on any node I am wondering how a node can verify that the pods from the apiserver are what a trusted user had in mind like image is signed a well defined behaviour defining subset of the pod spec is signed the exec is done via an e2e connection and the private key client side matches the public key provided as part of the signed spec The use case I have in mind You want to define a set of nodes to process credit card information or other security critical data e g from volumes that can only be accessed on this set of nodes You can use taints tolerations to guide the scheduler that nothing else is scheduled there If we can limit the tolerations that a user can apply we can even enforce that some day But if you are able to compromise the pod spec of pods scheduled you have rooted the system A way out would be to add a signature to pod specs such that the kubelet of those high security nodes can verify that the pods are trusted Then the apiserver scheduler controllers are not the weakest link anymore On Mar 11 2017 at 1 29 PM Dr Stefan Schimanski notifications wrote The use case I have in mind You want to define a set of nodes to process credit card information or other security critical data e g from volumes that can only be accessed on this set of nodes You can use taints tolerations to guide the scheduler that nothing else is scheduled there If we can limit the tolerations that a user can apply we can even enforce that some day But if you are able to compromise thde pod spec of pods scheduled you have rooted the system A way out would be to add a signature to pod specs such that the kubelet of those high security nodes can verify that the pods are trusted Then the apiserver scheduler controllers are not the weakest link anymore kubelet workload root ca some root ca crt Yes that s untrusted masters I think someone has a write up All the usual difficulties in signing content apply but it would be a good start Once you have that you can reject exec connections that are also unsigned Logs are pretty difficult it s better not to allow them Port forward has to be rejected etc The more things you add there the closer you get to wanting a separate set of apiservers or at minimum some edge proxy that is distinct I am interested in working on this project Outside GSoC for sure I ve been a Kubernetes user and I am a contributor to the Docker project I guess we need a general proposal for the untrusted master scenario and or find the old discussions if they exist somewhere as mentioned Securing the exec command is only one part of a holistic solution If you want to work on that it s of course very welcome sig api machinery sig node If you agree I too would like to work on this I did this yr s GSoC with PSF and have been a k8s user but would now like to contribute too So I understand the issue at the surface but would like to know the present state and interest? thanks Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale remove lifecycle stale lifecycle frozen  
CVE 2017 1000056 PodSecurityPolicy admission plugin authorizes incorrectly A PodSecurityPolicy admission plugin vulnerability allows users to make use of any PodSecurityPolicy object even ones they are not authorized to use CVE CVE 2017 1000056 Fixed in in Fixed in release 1 5 branch in Fixed in master in Only Kubernetes 1 5 0 1 5 4 installations that do all of the following Enable the PodSecurityPolicy API Enable the PodSecurityPolicy admission plugin Use authorization to limit users ability to use specific PodSecurityPolicy objects kubeadm and GKE do not allow enabling PodSecurityPolicy in 1 5 so are not affected by this vulnerability kube up sh and kops do not enable PodSecurityPolicy by default so are not affected by this vulnerability A modified kube up sh or kops deployment could have enabled it A user that is authorized to create pods can make use of any existing PodSecurityPolicy even ones they are not authorized to use 1 Export existing PodSecurityPolicy objects 2 Review and delete any PodSecurityPolicy objects you do not want all pod creating users to be able to use For example 3 After upgrading to 1 5 5 re create the exported PodSecurityPolicy objects Kubeadm and GKE do not allow enabling PodSecurityPolicy in 1 5 so are not affected by this Kube up sh and kops do not enable PodSecurityPolicy by default A modified kube up sh or kops deployment could have enabled it  
 Per container automountServiceAccountToken Consider an app that is exposed publicly and for some reason needs to be able to modify cluster state The more complex the app the higher the changes of it including a vulnerability which may expose the service account s token Splitting the part that talks to the k8s API server into a separate sidecar container having the app communicate to the sidecar through localhost and mounting the service account token only into the sidecar would make the whole system much more secure AFAICT it is currently not possible to toggle automouting for individual containers just for the pod as a whole although I think I remember someone mentioning somewhere that if you mount some other volume at the serviceaccount dir the token won t be mounted I m not sure about it being much more secure once you re in the same process space sig auth feature requests Maybe but if for example the main container only has a path traversal vulnerability allowing the token to be exposed to an external client the proposed feature would make sense right? And even if we forget about the security aspect it makes sense for an setting to primarily be at the container level since that s where you mount volumes Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close Prevent issues from auto closing with an comment If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
endpoints for daemonset in host network not ready and inconsistent with pod IPs kind bug After deploying the prometheus node exporter as daemonset service with these manifests the node exporter gets deployed to all nodes as expected The pod IPs match the host ips which I assume is expected given the node exporter runs in host networking namespace So far so good The problem is that all but one endpoints are not ready and they are all inconsistent with their pod IP This leads prometheus to miss one instance and apply wrong labels This could lead to misinterpretation of monitoring results in this particular case and probably even catastrophic failure in other cases I expected the endpoint IP match the pod ip This is a pretty much vanilla GKE cluster with applied The node exporter is running in host networking and pid namespace Kubernetes version 1 6 4 Cloud provider or hardware configuration Removed Important This issue was missing the label for more than 7 days details summary Help summary ul li a href Additional instructions a li li a href Commands for setting labels a li ul details We are experiencing the same issue on GKE with a test cluster In our case we have a preemptible instance pool where the nodes are terminated at least every 24 hrs They retain the same instance name but fairly certain the ephemeral ip address changes each time Can you share you the output of the following command? I have a theory here This bug happens in the following conditions 1 k8s cluster has service pointing to pods using host network Pods could be managed by daemonset 2 Instance internal IP changed due to whatever reason Validation for UpdateEndpoints uses EndpointAddress IP as key and check if its node has changed For example something like this may happen 1 Originally Node1 has IP1 and Node2 has IP2 Pod1 on Node1 with hostnetwork has IP1 and Pod2 on Node2 has IP2 2 Due to some disruption instance internal IPs are changed Node1 gets IP2 and Node2 gets IP1 That means Pod1 gets IP2 and Pod2 gets IP1 3 k8s endpoint controller will try to reconcile the existing endpoints object And swap the EndpointAddress of Pod1 and Pod2 However the validation logic uses the IP as key and checks if corresponding node name changed Then the endpoints object end up stucking in this error state I can t share from my current cluster but I should be able to re create this fairly easily in a clean cluster I should be able to do that over the weekend would it be possible to also post the repro instructions? sorry I got caught up with other things and haven t had a chance to work on this Below is my plan to reproduce but I haven t tried it yet 1 create GKE cluster default instances 2 add pool of preemptible nodes 3 install prometheus via helm chart 4 check time series for up 0 in prometheus This may take a while since the preemptible nodes are typically only restarted 1x 24hrs So I was also going to manually stop and restart the nodes Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale remove lifecycle stale lifecycle freeze Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close bot Closing this issue details In response to Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details and do you still face this problem ? I am also facing this issue and running as well on GKE I am using k8s version  
Downward API support in volume subPath This form is for bug reports and feature requests ONLY If you re looking for help check and the kind feature We would like a way to dynamically generate host paths when mounting volumes The feature creates directories on demand but the names assigned to those directories are static Supporting the downward API variables would provide a good way to share storage and avoid collisions Centralized log storage is one use case For example Another option Over time the host storage would look something like this and the containers would not need to change any of their logging logic There are no sig labels on this issue Please by br mentioning a sig br e g for API Machinery br specifying the label manually br e g for sig scalability br br Note method will trigger a notification to the team You can find the team list and label list sig storage I m not sure I understand what you gain by using instead of directly To use you need the extra scaffolding to support a map instead of a string note that the key was changed from to to align with ENV and pattern If you support then it s still a string and can use the variable substitution work that is already in place for other sections of the file Ah I see I misread your example as saying that was the existing solution and you wanted to move to with an env var Are there other cases where we plumb pod env vars to pod spec fields today? wdyt about this? I m wary of accidentally creating a templating solution You can use expanded env vars in command line and args lines only So that s how you can create a unique subpath using an init container with a command line symbolic link but that solution is messy I d rather not selectively allow env var expansion depending on the subfield of a config that would be pretty confusing to users I d prefer the subPathFrom solution if you just want this for subPaths Ofc we d have to caution users not to reference fields that don t contain valid filenames expansion is already implemented for args and env vars how expanding it for another field makes it confusing? doesn t work very well as it doesn t allow things like which is a real use case and pain point at least for us If you only extend it to work in the subPath case people will expect it to work everywhere and be confused that it doesn t It would be interesting to have env var expansion work in arbitrary fields of the container spec and I think it s worth discussing with a larger audience I d recommend writing a brief design doc in a PR against the community repo It is a significant API decision IMO General templating has been discussed a lot before and we still don t have an implemented solution But since this is a far more specific use case and you re only resolving variables for containers it might get more traction By adding the subpath doesnt in my opinion make it more confusing because it is already very restricted to only 2 elements The current way of using an init container is much more confusing for an end user donnelly thoughts? Im trying to do the same as what this issue is talking about But I dint understand s comments about using an init container to solve this I have a init container that can reference the pod s name and create a subdirectory in NFS specific to the pod but how do I mount that subdirectory into my container Should I use a postStart script to symlink the directory? Thanks I tried a similar approach earlier which led to an error of because the that you mentioned wasnt happening in my case i e once the symlink was created in the init container when actually doing the volumemount subPath it tried to create a path called and failed with error Ill retry with your configuration and see what happens I worked around it with a postScript that does something like and exposing PODNAME as env variable Thanks for the update much messing about can be alleviated with an environmental expansion hence my PR 49388 Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale remove lifecycle stale sig storage feature requests cc Sorry the trick will no longer work due to We will have to reconsider if how to support this feature request as the subpath handling is quite complicated now Which trick are you referring to the symbolic link in the init container ? We found that out by trying to go above 1 9 3 and they all failed But we believe that the PR removes all that need anyway so not sure why the PR won t work as the e2e tests within it are passing Yes actually the trick could still work if you use a hard link instead of a symlink As long as the link you re creating is within the same volume then we will allow it Anyway regarding this feature we need to have further API discussions because as points out this is straddling the line with templating and could have broader implications I can bring it up in the next sig storage meeting For this particular use case of logging have you considered using a distributed logging pipeline with fluentd and elasticsearch? Yes It isnt quite that simple Our legacy apps write a number of log files not just single streamed stdout so we need to isolate them into a file system structure and get splunk to log them If we have multiple pods running on a single instance then the file systems have to be uniquely partioned Also it isn t really straddling templating There is already environment expansion in command and args This is just extending that same principle into the volume mount It s really quite a small pr change to be quite fair This is definitively needed pods for the same pod template deployment could land on the same host and this would result in a path clash w o this option Also some logging is so voluminous that splunk elk fluentd would just be too costly Has there been any progress on this? We are totally blocked on moving from k8s 1 9 3 until we get some sort of resolution Thanks We didn t have time in yesterday s sig meeting to discuss this unfortunately Are you not able to use the workaround of hard links or relative symlinks in the init container? Given that we just had a huge vulnerability in the subpath feature and this feature is also touching the same area I want to be more conservative and at the very minimum have this feature go through alpha beta ga feature gating and have the design go through security review as well I do not want to use links or otherwise as this is an entire bodge We have 100s of apps and I do not want to start redeploying our entire product base by changing our init containers especially as they took us a lifetime to make work in the first place  
Also include namespace in encoding for append hash? In a comment on yesterday suggested including the name and namespace in the hashes While the current implementation includes the name it does not include the namespace We should consider including the namespace as well because it makes dictionary attacks less portable The tradeoff is that this will be a backwards incompatible change to kubectl cc Including namespace SGTM We should make the change and patch 1 8 Including namespace SGTM We should make the change and patch 1 8 the namespace is not always available when generating the object Another alternative that would be acceptable for user namespace Secrets is to include the object name prefix and namespace in the hash Aren t the namespaces the ones most likely to be attacked in this way? It seems like a user provided salt would be more effective Good point The main requirement is that it be deterministic rather than randomly generated Are we concerned about the data being part of the Secret? If not then adding any additional keys to the secret would work It scares me that we are relying on this hash to be secure Dictionary attacks are not practical against actually random secrets e g a sufficiently large tls key Adding a namespace or salt to the hash improves the security negligibly Database passwords might be dictionary attackable sig auth feature requests What about adding from literal to all types of secrets and document that users who are concerned about this should add their own salt? What about adding from literal to all types of secrets and document that users who are concerned about this should add their own salt? Why not a specific flag for salting the hash without making it part of the secret data? I would have thought exposing the salt in the secret would be a non goal AIUI there were 2 potential issues 1 The resource names could be less secret than the contents For instance they may occur in logs They may be discoverable by API probes Discovery of the name is needed in order to perform the dictionary attack 2 With the name the dictionary attack is potentially possible only because all the keys and structures of the values are known for the common types of secrets I think the question is what additional kind of uncertainty is required to mitigate the concern about the potential vulnerability to this type of attack Would a fixed length random string with a well known key name be sufficient? It seems no different than the other parts of the secret except in the case of a short password where it would make the amount of data significantly larger In any case if the contents of the secret were exposed I think the game is over anyway In any case if the contents of the secret were exposed I think the game is over anyway Possibly I m trying to think how this would be used A site specific salt used for all secrets managed by a config management system is a possibility and would mean visibility to one secret would give access to the salt possibly used for other secrets That said I don t feel that strongly either way Ok not stored in secret not on command line not fixed length for maximum paranoia I m not clear on what we ve decided to do here Is adding to kubectl still in the cards? If we re worried about the command line being logged and leaking the salt we could have point to a file that contains the salt Bump Better to resolve this sooner than later esp w kinflate and co on the way Ok not stored in secret not on command line not fixed length for maximum paranoia I guess the suggestion is just that users do and add a file with a salt in it? If that s satisfactory can we document it and close this issue? Yes a file with the salt The secrets would need to come from somewhere also so I don t think the salt is very different has been working on a solution for kinflate we could have salt point to a file that contains the salt As I understand it we would add this flag to and kinflate will generate the salt from a declared command I think that solves all the problems mentioned in that issue PS Just a quick note on how handles secret we generate secret content from the stdout of commands ran at creation time the goal is to be able to generate secrets with things like which would only work for privileged machines users We can definitely generate the salt in the same way If we can avoid a flag that s even better This is possible in the and cases because you can just include the salt in the secret data I m not sure about the more specific subcommands can still be if the salt is taken from the stdout of another command since in that case it won t show up on the command line Offline chat w regarding how we warn users to actually start using a salt if we don t have a new flag since the salt is mainly to pad the size of the secret kubectl should warn people to add a salt if the size of the secret is below some arbitrary threshold what would you say is a good size threshold? lists as an optional flag does not but it sounds like these are big enough that its a non issue for secret stuff  
Nginx ingress controller closes chunked encoding requests on when configuration reloads This form is for bug reports and feature requests ONLY If you re looking for help check and the This is a BUG REPORT kind bug sig network sig network bugs We found that if you have a long request response connection such as a chunked encoding response the request is prematurely truncated when the ingress controller reloads the nginx configuration We expected the connection to remain open 1 Run an upstream server with an endpoint that responds with a long lived chunked response For example one chunk every second for 100 seconds The language shouldn t matter but I am using an Elixir Phoenix app with this action 2 Make a request with something like time curl raw localhost test stream H Host bar gigalixir com 3 After a random amount of time the connection will close and curl will print curl transfer closed with outstanding read data remaining You can reproduce the issue by manually running the following inside the ingress controller container and waiting 10 seconds nginx s reload It appears as though the nginx worker processes drain requests for 10 seconds when reloading before cutting the connections which in my case is not long enough I m not sure what the proper fix is for this but in my case I am tailing logs through chunked encoding so I need the connection open basically indefinitely but at least an hour or so Kubernetes version Cloud provider or hardware configuration will not retry POST request right? And there is only one replica of the upstream server Following is snippet of nginx conf in my env So nginx will not retry POST request right? No unless you enable And there is only one replica of the upstream server If you have only one server not sure it makes sense to retry Please keep in mind the default value is the default  
Volume from a remote archive kind feature Right now it s possible to have volume which is nice It mounts an empty directory and clones a git repository into it for a pod to use It will be wonderful to be able to have a volume from a remote archive Kubernetes can mount an empty directory download an archive by URL and unarchive it for a pod to use Config can look like this It can support different formats like Tar TarGz and Zip sig storage enmmm Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Since can be a potential security vulnerability having might be a good thing Please don t close the issue Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten remove lifecycle stale Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close  
Kubelet API to lookup pod name from socket The kubelet should expose an API that allows a node agent to lookup a pod from an inet socket This information could be exposed through the kubelet rest handler or through a hook in the network setup teardown This API will be used for secure caller ID by node agents so the information exported must be up to date and correct sig auth sig node kind feature There are a number of plugin vendors that reverse engineer our cgroup or namespace setup to derive the source pod container from something like the PID A more general version of this API might allow looking up the Pod container by various resources We need to identify what constructs we want to attach identity to Attaching identity to cgroups precludes us from implementing resource models where a node agent takes resources from pods in exchange for providing a service For example I could imagine a log shipping node agent that creates a thread per pod and drops those threads into the pod s cgroup Do we want to keep our options here? Are there situations in which an unprivileged agent would need to enter a pod s network namespace? Container ID is also relevant but much further out The kubelet should expose an API that allows a node agent to lookup a pod by it s IP Could you provide a use case for this or add a link to a relevant feature? I m not sure what mean in this context This information could be exposed through the kubelet rest handler or through a hook in the network setup teardown I don t quite get how the network setup tear hook is going to work do you mean supporting hooks through plugins or kubernetes API? sig auth kind feature milestone v1 11 priority important longterm status approved for milestone Milestone Issue Labels This issue requires label changes If the required changes are not made within 3 days the issue will be moved out of the v1 11 milestone Must specify exactly one of or details summary Help summary ul li a href Additional instructions a li li a href Commands for setting labels a li ul details Not sure this belongs to sig auth is it planned committed for 1 11? Sorry making my way through the backlog Removing this for now since we still need a use case Identity integrations workload APIs over either UDS or localhost with no standard mechanism for mapping caller to pod While UDS works reasonably well today it will become problematic as container runtimes with stronger isolation become more prevalent I would like to standardize this mechanism sooner rather than later I d think that sig auth node networking are all stakeholders cc I m curious why this has to be a kubelet API The kubelet doesn t really store this information it basically just updates the field in the apiserver What is a case where talking to the apiserver doesn t work? A second question how should this API handle host network pods where multiple pods have the same IP? I m curious why this has to be a kubelet API It could work as long as the read is consistent It feels like an unnecessary hop since the source of truth is local The kubelet doesn t really store this information it basically just updates the podspec IP field in the apiserver It stores this information in memory and it s already available from the pods handler Whether the info is consistent or not is an implementation detail but we would be relying on consistency to maintain security how should this API handle host network pods where multiple pods have the same IP? I was probably too prescriptive in the original issue description I changed the to We can either lookup pod by source IP of inet socket This attaches identity to the pod s network namespace Anything in the network namespace would be running as the identity of the pod This won t support host network pods I think it s probably OK to treat hostnet pods as the host lookup pod by pid gid of inet socket Attaches identity to the pid namespaces of the pod Would support host network pods but might be more expensive Wouldn t support host pid pods Anything running in the host pid would be running as the host Well when you put it that way just adding one more cache to podmanager is easier It stores this information in memory and it s already available from the pods handler Whether the info is consistent or not is an implementation detail but we would be relying on consistency to maintain security Even if the information is consistent this sounds ripe for a race condition vulnerability lookup pod by pid gid of inet socket Attaches identity to the pid namespaces of the pod Would support host network pods but might be more expensive Wouldn t support host pid pods Anything running in the host pid would be running as the host I don t think this will work for Kata Containers and may not work for Windows containers We currently have a FlexVolume Unix Domain Socket solution prototyped and being tested This attaches identity to a mount namespace It s not the cleanest solution but works fine on Linux IMO one of the main arguments for building a Kubelet API solution is that it can enable non Linux use cases See CNI GET command cc The CNI GET command takes the container ID as a parameter which seems to beg the question Alone it implements the reverse lookup of what we want here but it could be used to confirm a forward lookup against a cached view of the pods on the node The combination of the kubelet s pod cache and CNI GET seem sufficient to satisfy our functional and security requirements Assuming the CNI GET API guarantees a consistent view Assuming the CNI GET API guarantees a consistent view can you clarify this a bit for me? Currently the kubelet exposes a view of it s pod cache If a process receives a request on localhost socket and wants to know which pod is on the other end it can 1 read src ip of the socket 1 read the pod cache 1 find a pod with a matching ip 1 assume the connection came from that pod The concern is that stale data in the cache mixed with pod ip address reuse may lead to the process misidentifying the calling pod Assuming that CNI GET is accurate at the time of invocation the process can instead 1 read src ip of the socket 1 read the pod cache 1 find a pod with a matching ip 1 call CNI GET with pod ip to get pod id 1 if pod id from CNI matches pod id from cache assume the connection came from that pod yeah that s not what CNI GET is for CNI GET is The original reason for GET was to remove the nsenter crap from kubelet and give kubelet a way to get pod IP network state on restart It s not meant to be a which is what I think you re looking for? Lastly CNI GET as currently implemented is also mostly a cache of the ADD response that CNI plugins can modify if they want to but we don t anticipate plugins doing much of that because of plugin chaining logistics So it s still a cache just not on in kubelet because a year or two ago when discussing this problem kubelet maintainers did not want to store that info there We ve since gotten checkpoints which is a reversal of that stance but still Besides if the kubelet cache of PodIP PodID is not correct and timely shouldn t that get fixed? Also at this time kubelet does not expect the PodIP of a pod to change over its lifetime While it technically can we discussed that at KubeCon Austin in the SIG Network sessions and decided it might be a worthwhile future goal but was not currently supported expected Instead I would suggest just using the kubelet pod cache but look at sandboxes not pods Find the active sandbox with the given IP address and then return that sandbox s Pod ID If kubelet can t correctly track which containers are using which sandboxes we ve got bigger problems than this API Also I don t think your proposal works for hostnetwork pods since every hostnetwork pod on the same node will share the host s IP address so there s no way to differentiate pods by IP Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale cc Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten We worked around this by interacting with CRI directly I ll close this until we need greater support  
CVE 2017 1002101 subpath volume mount handling allows arbitrary file access in host filesystem This vulnerability allows containers using with any volume type to access files directories outside of the volume including the host s filesystem Thanks to Maxim Ivanov for reporting this problem Kubernetes 1 3 x 1 6 x Kubernetes 1 7 0 1 7 13 Kubernetes 1 8 0 1 8 8 Kubernetes 1 9 0 1 9 3 Clusters that allow untrusted users to control pod spec content and prevent host filesystem access via hostPath volumes using PodSecurityPolicy Clusters that make use of with untrusted containers or containers that can be compromised A specially crafted pod spec combined with malicious container behavior can allow read write access to arbitrary files outside volumes specified in the pod including the host s filesystem This can be accomplished with any volume type including emptyDir and can be accomplished with a non privileged pod Prevent untrusted users from creating pods or with PodSecurityPolicy Fixed in v1 7 14 by 61047 Fixed in v1 8 9 by 61046 Fixed in v1 9 4 by 61045 Fixed in master by 61044 In addition to upgrading PodSecurityPolicy objects designed to limit container permissions must completely as the allowedHostPaths feature does not restrict symlink creation and traversal Future enhancements are required to limit hostPath use to read only volumes or exact path matches before a PodSecurityPolicy can effectively restrict hostPath usage to a given subpath Status and availability of fixes for regressions in subPath volume mount handling are tracked in There are no sig labels on this issue Please add a sig label details A sig label can be added by either 1 mentioning a sig e g to notify the contributor experience sig OR 2 specifying the label manually e g to apply the label Note Method 1 will trigger an email to the group See the The in method 1 has to be replaced with one of these Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details Milestone Issue This issue is marked as and must be updated every 1 day during code freeze Example update details open summary Issue Labels summary Issue will be escalated to these SIGs if needed Never automatically move issue out of a release milestone continually escalate to contributor and SIG through all available channels Fixes a bug discovered during the current release details details summary Help summary ul li a href Additional instructions a li li a href Commands for setting labels a li ul details This article states that Kubernetes 1 7 0 1 7 13 Does this mean all versions between 1 7 0 and 1 17 13 are vulnerable? For example is 1 17 6 vulnerable? anything between 1 7 0 and 1 7 13 are vulnerable including 1 7 6  
CVE 2017 1002102 atomic writer volume handling allows arbitrary file deletion in host filesystem This vulnerability allows containers using a secret configMap projected or downwardAPI volume to trigger deletion of arbitrary files and directories on the nodes where they are running Thanks to Joel Smith of Red Hat for reporting this problem Kubernetes 1 3 x 1 6 x Kubernetes 1 7 0 1 7 13 Kubernetes 1 8 0 1 8 8 Kubernetes 1 9 0 1 9 3 Clusters that run untrusted containers with secret configMap downwardAPI or projected volumes mounted A malicious container running in a pod with a secret configMap downwardAPI or projected volume mounted can cause the Kubelet to remove any file or directory on the host filesystem Do not allow containers to be run with secret configMap downwardAPI and projected volumes Fixed in v1 7 14 by 60516 Fixed in v1 8 9 by 60515 Fixed in v1 9 4 by 60258 Fixed in master by 58720 Secret configMap downwardAPI and projected volumes will be mounted as read only volumes Applications that attempt to write to these volumes will receive read only filesystem errors Previously applications were allowed to make changes to these volumes but those changes were reverted at an arbitrary interval by the system Applications should be re configured to write derived files to another location There are no sig labels on this issue Please add a sig label details A sig label can be added by either 1 mentioning a sig e g to notify the contributor experience sig OR 2 specifying the label manually e g to apply the label Note Method 1 will trigger an email to the group See the The in method 1 has to be replaced with one of these Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details I noticed that the patch doesn t check for hard links I don t know enough about this particular case if it d add any risk thought I let you all know thanks for reviewing the patch Hard links are fine in this case because they cannot cross mounts In general if you see any further issues please report it following the sig storage bugs Forcing configmap and secret mounts to be readonly is not acceptable I should have the option to make it writeable Also the ReadOnlyAPIDataVolumes feature gate will be removed so I m forced to change my application because of this  
configMap and secrets volumeMount are always mounted readOnly in 1 9 6 kind bug After upgrading from 1 9 4 to 1 9 6 configMap and secrets volumes are always mounted ReadOnly even when the deployment specs don t set the option and show the mount ad rw Deployment specs and kubectl describe show RW Docker inspect and actual mountpoint inside of the running container show RO Mountpoints should be RW as they used to be at least up to 1 9 4 Have a 1 9 6 cluster apply the following spec and verify that mounts are RO rather than RW Kubernetes version Cloud provider or hardware configuration Baremetal and KVM Vms OS Kernel Install tools Matchbox Others It used to work fine on 1 9 4 i upgraded to 1 9 6 due to and problem started Tested with minikube 1 9 3 and it works as expected the configmap and secrets are mounted RW sig storage This was intentional More details If you really want or to be then you can disable the feature gate making atomic writer volumes readonly was part of the fix for Fix impact Secret configMap downwardAPI and projected volumes will be mounted as read only volumes Applications that attempt to write to these volumes will receive read only filesystem errors Previously applications were allowed to make changes to these volumes but those changes were reverted at an arbitrary interval by the system Applications should be re configured to write derived files to another location close Ok Great i must have missed it in the changelog Already did work around it so all good thanks what is the work aboud? Thanks My approach is to copy the config map to volume if I need rw capabilities I don t agree with this I should have the option to control readOnly flag In fact when I describe the pod the confmap and secrets do show rw Hello I totally agree with config maps being readonly But it was extremely surprising for this to happen within a minor version upgrade Currently we use GKE with automatic upgrades since these sort of minor version upgrades seemed safe but this time the upgrade broke an old version of the grafana helm chart we were using and indeed going from rw to ro is a breaking change IMO So just wondering is there a semantic versioning policy for Kubernetes to prevent breaking changes on minor upgrade? If there is I think this issue was closed without resolution if not no worries It does but unfortunately the policy was ignored in this case But it was extremely surprising for this to happen within a minor version upgrade Currently we use GKE with automatic upgrades since these sort of minor version upgrades seemed safe but this time the upgrade broke an old version of the grafana helm chart we were using and indeed going from rw to ro is a breaking change IMO As noted in the breaking change was required to close a significant security vulnerability you should be able to control the readOnly option and accept the security vulnerability Default should be set to true if this is preferred but you should be able to take the risk with this breaking change Thanks for the info I can see the security implications here But considering the fact that it is completely breaking I still wonder whether a minor version update is appropriate for it even when there is a fallback It might be but needs to be coordinated with the major cloud providers I follow GKE release notes continuously but there was no mention that an auto upgrade would introduce breaking changes This doesn t seem natural to me but perhaps GKE s auto upgrade process can be blamed Default should be set to true if this is preferred but you should be able to take the risk with this breaking change Setting the feature gate allows just that as noted in the release notes ReadOnlyAPIDataVolumes true false Running k8s 1 10 3 is being nice about it but I agree with him completely and probably more strongly Breaking your contract with users is about the worse thing an open source project can do Breaking change in a patch version is never the right choice except in very extreme situations This should have been opt in not opt out Release notes are not good enough either when the versioning docs imply we can do auto updates to patch versions safely like GKE does Breaking change in a patch version is never the right choice except in very extreme situations I agree with you Preventing data loss and closing severe security vulnerabilities are among the few reasons such a break would even be considered and both of those factors were part of this decision I just wonder whether the proper communication was taken with the cloud vendors It s a hard decision to make a breaking change with a patch release but at least can be better if users are made aware at least in e g the GKE release notes If there s a process for such a notification via the cloud vendors I think it will allow people to worry less when using auto upgrade Or is the ball in their court for establishing such a process? I m dismayed that this option was broken while still silently allowing If this feature is now unsupported the API machinery should now refuse a pod spec when a secret or configmap has and it won t be enforced please see last comment do we need to do anything by the time we ship 1 11? do we need to do anything by the time we ship 1 11? No rejecting previously persisted API objects as invalid would cause far more compatibility problems It would prevent all updates to those objects including those required to delete them Additionally serialization of that particular field does not currently distinguish between and Changing that could potentially be done in the future but would require significant care to avoid invalidating already persisted data Fix impact Secret configMap downwardAPI and projected volumes will be mounted as read only volumes Applications that attempt to write to these volumes will receive read only filesystem errors Previously applications were allowed to make changes to these volumes but those changes were reverted at an arbitrary interval by the system Applications should be re configured to write derived files to another location Wonder what happens if the app can not ? like in case of hosting a third party app that expects to find a config file in specific location and that location only? Same goes with writes and creating directories in that specific path I just got bit hard by this especially the silent failure with the config For those encountering this I would recommend using an init container to copy over the contents of the secret config map directory into a shared empty dir Hello I still see the feature gate in the release 1 11 it should not be removed? and just to clarify this readonly will be a default? cant we set that to false? thanks I still see the ReadOnlyAPIDataVolumes feature gate in the release 1 11 it should not be removed? and just to clarify this readonly will be a default? cant we set that to false? that is still present in 1 11 only to satisfy the deprecation period for kubelet CLI flags it will be removed in 1 12 ok thanks asking because the comment says 1 11  
OpenSSL vulnerability need upgrade openssl to openssl 1 1 0f 3 deb9u2 version OpenSSL Security Advisory Details External Disclosures CVE 2017 3736 CVE 2017 3738 External Disclosure URL specified below Current hyperkube image includes OpenSSL 1 1 0f version it need update to openssl 1 1 0f 3 deb9u2 version which has included CVE 2017 3736 CVE 2017 3738 fix sig node Issues go stale after 90d of inactivity Mark the issue as fresh with Stale issues rot after an additional 30d of inactivity and eventually close If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle stale Stale issues rot after 30d of inactivity Mark the issue as fresh with Rotten issues close after an additional 30d of inactivity If this issue is safe to close now please do so with Send feedback to sig testing kubernetes test infra and or lifecycle rotten Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close bot Closing this issue details In response to Rotten issues close after 30d of inactivity Reopen the issue with Mark the issue as fresh with Send feedback to sig testing kubernetes test infra and or close Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details  
Support for custom tls cipher suites in kube proxy This form is for bug reports and feature requests ONLY If you re looking for help check and the If the matter is security related please disclose it privately via Uncomment only one leave it on its own line kind bug kind feature SSL Medium Strength Cipher Suites Supported Add option to as in PR for api server and kubelet Kubernetes version 1 10 Cloud provider or hardware configuration Any OS Any Kernel Any Install tools Others There are no sig labels on this issue Please add a sig label details A sig label can be added by either 1 mentioning a sig e g to notify the contributor experience sig OR 2 specifying the label manually e g to apply the label Note Method 1 will trigger an email to the group See the The in method 1 has to be replaced with one of these Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details Vulnerability Description Synopsis The remote service supports the use of medium strength SSL ciphers The remote host supports the use of SSL ciphers that offer medium strength encryption Nessus regards medium strength as any encryption that uses key lengths at least 64 bits and less than 112 bits or else that uses the 3DES encryption suite Note that it is considerably easier to circumvent medium strength encryption if the attacker is on the same physical network Port 6443 Here is the list of medium strength SSL ciphers supported by the remote server Medium Strength Ciphers Where should I fix the weak ciphers ? I did it for apache and tomcat but does not know where to fix for kubernetes Thx 6443 is the apiserver try to add I am closing this issue as i have no Vulnerabilities set on and solved the issue Thanks Thx much Pavel On Tuesday July 31 2018 12 35 23 AM PDT Pavel Druyan notifications wrote 6443 is the apiserver try to add tls cipher suites TLS ECDHE RSA WITH AES 128 GCM SHA256 You are receiving this because you were mentioned Reply to this email directly view it on GitHub or mute the thread  
Move coredns coredns to k8s gcr io for 1 10 and 1 9 Head is using k8s gcr io It would be great if you could also update the 1 10 and 1 9 release branches to do the same so we can vulnerability scan those images kind bug sig auth To clarify we ll need gcr io images for CoreDNS 1 0 1 and 1 0 6 for k8s v1 9 and v1 10 respectively? Can we move 1 10 and 1 9 to coredns version 1 1 3 Pushing image of CoreDNS to gcr io can be done right now only by He is out of office for 2 weeks from now Once the images are pushed I guess we could update the manifest of CoreDNS to to point to the repository gcr io in order to retrieve the images That would be for future releases of k8s 1 9 and k8s 1 10 is that what you expect ? Or you just need to have the 2 images in gcr io for scan purpose ? That would be for future releases of k8s 1 9 and k8s 1 10 Why not just move to coredns 1 1 3 at that time? Why continue to use an old CoreDNS? 1 0 x 1 1 X major version change possible custom configuration incompatibilities? I guess we could move to 1 1 3 but that needs validation first against the corresponding K8s versions Can we do it using PR and presubmit tests ? It would get validation in the e2e tests however I think the minor version change means that we introduced some backward incompatible changes between those versions Which means we could break deployments if people have customized their configurations to use a deprecated plugin option So not a good idea to upgrade Ping This is still causing pain for image vuln scanning assign cc we ve been discussing this on slack and fighting with manifest tool and creds from I m happy to make this happen but I can t get the tooling to work right the manifest tool push 401s due to GCR and auth images should be in now 1 0 1 1 0 6 It looks like 1 9 and 1 10 are still using which we cannot vuln scan Any chance of someone changing those references in 1 9 10 to as it appears in 1 11 ? 1 9 is unsupported by the community With 1 13 this quarter 1 10 will go out of support too According to and 1 9 is no longer supported CoreDNS image is available as gcr io in release 1 10 This issue has been resolved close Closing this issue details In response to According to and 1 9 is no longer supported CoreDNS image is available as gcr io in release 1 10 This issue has been resolved close Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details  
Update kube addon manager base Alpine image to 3 8 1 kind feature Alpine fixed a RCE vulnerability in apk in 3 8 1 Update the base image used for addon manager to pick it up Note I d send a PR if it depended directly on alpine image not bashell alpine There are no sig labels on this issue Please add a sig label by either 1 mentioning a sig e g to notify the contributor experience sig OR 2 specifying the label manually e g to apply the label Note Method 1 will trigger an email to the group See the The in method 1 has to be replaced with one of these details Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details assing Ping  
Kubernetes URL has Cross Site Scripting vulnerability kind bug Cross Site Scripting attacks occur when data enters a Web application through an untrusted source most frequently a web request and the data is included in dynamic content that is sent to a web user without being validated for malicious content The malicious content sent to the web browser often takes the form of a segment of JavaScript but may also include HTML Flash or any other type of code that the browser may execute It is observed in server error user input reflects without encoding Note We have observed that the application responds with the 404 page with the injected JavaScript and content type is text html throughout the application REQUEST GET api v1 namespaces kube system pods heapster d5c99f75b fj7jxium3i 3cimg 20src 3da 20onerror 3dalert 3ez24qy HTTP 1 1 Host 9 30 220 229 8001 User Agent Mozilla 5 0 Gecko 20100101 Firefox 62 0 Accept application json Accept Language en US en q 0 5 Accept Encoding gzip deflate Referer content type application json x requested with XMLHttpRequest origin Cookie JSESSIONID 0000Fkx9WjTGJQYtQRSatQPPl57 79f75479 6cf0 484e 9535 2b48d55abc78 WAS n581030044 zlzcyOWYBBxoS2hklvvOl9o78ZhTFA5mQf19wYofFtNpn0 Jka0Pi2KJWXt0T1uGAO VfxNLZ0 jf3Ilhg4IKWCXG3ZNMEVI73ui 3bOmQc1GtbjtbtlK7MKMyETIvtKMDKM4L49 CON79t58n9q CmvF LWu6fR70KpQmZwR7H86JKlOT mGPwhguv0x2t5GgyO6wHiNVAOb3QM63KL4XI5AhX kWep6 er5oPERu4hI GyTxfncrVnmjXtVCO 08zzjSClpkncokPFeCnR2O4fEW2z4okH5krsnU2yK77f1vDwWbwS mh0tljO HO6zH e cfc acs auth cookie f1cb51679d5f559617782162f1140f1b131967d659e5719f7cfeecdce02885d35559cf121a69a6 a5da6373ebd0d59bc1cedf02375b880fbc0e5f9b296ef1100be53902e6f4d32631d94b0db5153bf487b 1b44dab56230152350c654387b26b65c1be26182b73355df35c1747332ae6ce521e84ed8df17695a71 dfbbd3744a69f130677d94b1498ba16f6445df901af9b6633ac55edd641a47dc519a9c3fb6524750b3fd 3a2507cd7df75ba1e33f2b4587d8a6c50782ef1ef5d45f16c9d15ce90b8c67eb9400b3bee8479b26a26 6bacfa7cb02b6805f6aa48290ac338efa3950c6b828146efeb8ee323dce2219b8d4612ddcdf333c65360 dd9cc96e5bc07bf460e5c42678d02fa28f8fc2d5f56e31fe7a32f887fecc9431d8a873603029143f9cfab6 e193ef5c1fd12dbfafe9f0fdbc37da0d75c181f51e9b2a3d6af148864e78eca49b2b67b75bf320162a883 7c2e255a32c5e202b2d15a490178fe93811c8b07578519de65b435631d13be35c218f560bd82fe4730 9be92d1ecfce0e97d564b1fdf7eec4f264f87c2b67b9a1bb4d13a8c39b16f35150416662b04a856ad8d1 29817f5a64010d5c9c7c24bb5b1f4597d58d1b2a26c01bc288094801295876c5be22c71f2bcbb86b524 65f35e9c9080a24f2a84ab90d5ffcbd46f689e719755389c820f97b19a2401a0cd5892539b3e72a3fe77 4d2908929835a4ca4a9fd0c6e5 cfc access token cookie f1cb51679d5f559617782162f1140f1b131967d659e5719f7cfeecdce02885d35559cf121a69a6 a5da6373ebd0d59bc1cedf02375b880fbc0e5f9b296ef1100be53902e6f4d32631d94b0db5153bf487b 1b44dab56230152350c654387b26b65c1be26182b73355df35c1747332ae6ce521e84ed8df17695a71 dfbbd3744a69f130677d94b1498ba16f6445df901af9b6633ac55edd641a47dc519a9c3fb6524750b3fd 3a2507cd7df75ba1e33f2b4587d8a6c50782ef1ef5d45f16c9d15ce90b8c67eb9400b3bee8479b26a26 6bacfa7cb02b6805f6aa48290ac338efa3950c6b828146efeb8ee323dce2219b8d4612ddcdf333c65360 dd9cc96e5bc07bf460e5c42678d02fa28f8fc2d5f56e31fe7a32f887fecc9431d8a873603029143f9cfab6 e193ef5c1fd12dbfafe9f0fdbc37da0d75c181f51e9b2a3d6af148864e78eca49b2b67b75bf320162a883 7c2e255a32c5e202b2d15a490178fe93811c8b07578519de65b435631d13be35c218f560bd82fe4730 9be92d1ecfce0e97d564b1fdf7eec4f264f87c2b67b9a1bb4d13a8c39b16f35150416662b04a856ad8d1 29817f5a64010d5c9c7c24bb5b1f4597d58d1b2a26c01bc288094801295876c5be22c71f2bcbb86b524 65f35e9c9080a24f2a84ab90d5ffcbd46f689e719755389c820f97b19a2401a0cd5892539b3e72a3fe77 4d2908929835a4ca4a9fd0c6e5 toggles 7B 7D Connection close HTTP 1 1 404 Not Found Server openresty 1 11 2 4 Date Wed 26 Sep 2018 16 00 07 GMT Content Type text html charset utf 8 Content Length 687 Connection close X Powered By Express ETag W Unexpected response code 404 from request GET system pods heapster d5c99f75b fj7jxium3i img src a onerror alert z24qy HTTP 1 1 Authorization Bearer Risk Assessment Fix Recommendation Perform sanitization on input strings Use an input validation strategy whitelist of acceptable inputs that strictly conform to specifications Reject any input that does not strictly conform to specifications or transform it into something that does User input should be HTML encoded at any point where it is copied into application responses All HTML metacharacters including and should be replaced with the corresponding HTML entities priority important soon sig api machinery cc Note We have observed that the application responds with the 404 page with the injected JavaScript and content type is text html throughout the application are you running through a proxy? I observe this response from the apiserver that is correctly setting and encoding the parameter also xref which merged recently cc The important is that I can inject JavaScript code and get response This is the injected JavaScript response I use kubernetes 1 11 3 and from this response to see it is a high risk if attackers inject other script What component is openresty? That appears to be turning escaped json content into unescaped html content My openresty version is 1 13 6 2 My openresty version is 1 13 6 2 Can you provide more details about your environment? How is kubernetes deployed what is openresty how are you accessing kubernetes via openresty? It appears that openresty is proxying to the the API and echoing raw API responses back to the browser as HTML which doesn t seem proper My openresty version is 1 13 6 2 Can you provide more details about your environment? How is kubernetes deployed what is openresty how are you accessing kubernetes via openresty? It appears that openresty is proxying to the the API and echoing raw API responses back to the browser as HTML which doesn t seem proper Yes openrestry is like nginx openresty is proxying to the the API and echoing raw API responses back to the browser as HTML Yes openrestry is like nginx openresty is proxying to the the API and echoing raw API responses back to the browser as HTML Then this issue should be reported as a bug against openresty close Closing this issue details In response to Yes openrestry is like nginx openresty is proxying to the the API and echoing raw API responses back to the browser as HTML Then this issue should be reported as a bug against openresty close Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details  
CVE 2018 1002105 proxy request handling in kube apiserver can leave vulnerable TCP connections With a specially crafted request users that are authorized to establish a connection through the Kubernetes API server to a backend server can then send arbitrary requests over the same connection directly to that backend authenticated with the Kubernetes API server s TLS credentials used to establish the backend connection Thanks to Darren Shepherd for reporting this problem CVE 2018 1002105 is in the following Kubernetes releases Affected components Kubernetes API server Affected versions Kubernetes v1 0 x 1 9 x Kubernetes v1 10 0 1 10 10 Kubernetes v1 11 0 1 11 4 Kubernetes v1 12 0 1 12 2 Note If you are using binaries or packages provided by a distributor you should contact them to determine what versions resolve this CVE Distributors may choose to provide support for older releases beyond the ones maintained by the open source project Affected configurations Clusters 1 6 x that run aggregated API servers that are directly accessible from the Kubernetes API server s network If there are aggregated API servers configured in a cluster the following command will return the names of the associated APIService objects Clusters 1 0 x that grant pod exec attach portforward permissions to users that are not expected to have full access to kubelet APIs Vulnerability impact An API call to any aggregated API server endpoint can be escalated to perform any API request against that aggregated API server as long as that aggregated API server is directly accessible from the Kubernetes API server s network A pod exec attach portforward API call can be escalated to perform any API request against the kubelet API on the node specified in the pod spec Mitigations This section lists possible mitigations to use prior to upgrading Note that many of the mitigations are likely to be disruptive and upgrading to a fixed version is strongly recommended Mitigations for the anonymous user aggregated API server escalation include suspend use of aggregated API servers disable anonymous requests by passing to the kube apiserver remove all anonymous access to all aggregated APIs Mitigations for the authenticated user aggregated API server escalation include suspend use of aggregated API servers remove all access to all aggregated APIs from users that should not have full access to the aggregated APIs Mitigation for the authorized pod exec attach portforward kubelet API escalation Remove pod exec attach portforward permissions from users that should not have full access to the kubelet API Detection There is no simple way to detect whether this vulnerability has been used Because the unauthorized requests are made over an established connection they do not appear in the Kubernetes API server audit logs or server log The requests do appear in the kubelet or aggregated API server logs but are indistinguishable from correctly authorized and proxied requests via the Kubernetes API server why doesn t kubelet s webhook authorization help here? Shouldn t it reject subsequent requests on still open TCP connection from APIServer? why doesn t kubelet s webhook authorization reject subsequent requests on still open TCP connection from APIServer? Because the kube apiserver kubelet connection was established with the kube apiserver s TLS credentials which are broadly authorized against the kubelet API The kubelet would authorize the kube apiserver to make that request and allow it Hmm I misunderstood how webhook authorization works then I thought that no requests are trusted in that mode and even for proxy requests initiator s credentials are passed by APIServer to kubelet which then comes back to apiserver for SubjectAccessReview So I thought scheme is like following Initiator APIServer kubelet APIServer But seems it is not the case kubelet APIServer the credentials authorized by the kubelet in that case are the apiserver s Thanks for the fix and the detailed summary In default configurations all users are allowed to perform discovery API calls that allow this escalation Just for clarification this vulnerability can t be exploited by anonymous users from of the cluster as long as we disable right? I m asking because my users tend to open up K8S API endpoint to the Internet occasionally but not always restricting accesses to the endpoint at L4 by the source IP Just for clarification this vulnerability can t be exploited from of the cluster as long as we disable right? Correct escalations start with a single authorized request through kube apiserver to the backend Preventing that mitigate the anonymous escalation Be aware that can disrupt normal operations of a cluster including load balancer and kubelet detection of kube apiserver health and kubeadm join flows that depend on public access to a config map containing cluster information in the namespace Initiator APIServer kubelet APIServer the credentials authorized by the kubelet in that case are the apiserver s Interesting are there any drawbacks of passing initiator s credentials all the way to kubelet? Would it allow to eliminate one more in the form of APIServer? Interesting are there any drawbacks of passing initiator s credentials all the way to kubelet? Exposing initiator s credentials broadly to the cluster is a non goal Additionally many API callers have non forwardable credentials The apiserver could forward information about the initiator to the kubelet but that information would have to be trusted Interestingly that is exactly the scenario exercised by kube apiserver aggregated API server connections where the kube apiserver acts as an authenticating proxy and the aggregated API server trusts the identity assertions sent to it by the kube apiserver Would it allow to eliminate one more in the form of APIServer? not really as long as kube apiserver is responsible for terminating and asserting authentication information to backends I m having a hard time understanding the full extent of this If the API is in a private network with RBAC enabled but still around would it still be vulnerable to this? If the API is in a private network with RBAC enabled but anonymous auth true still around would it still be vulnerable to this? The first step in the escalation is a properly authorized request to the kube apiserver that gets proxied to a backend server A malicious request must be able to reach the kube apiserver for the vulnerability to be exploited That s what I thought thanks so much for taking the time to answer so quickly Thanks for the clarification Hi I want to know why the issue isn t fixed in 1 9 x? We have a lot of clusters running the 1 9 x k8s Pardon stupid question but is Heapster considered aggregated API server same as metrics server? Hi I want to know why the issue isn t fixed in 1 9 x? We have a lot of clusters running the 1 9 x k8s The Kubernetes project supports the most recent three minor releases with bug and security fixes The currently supported releases are 1 10 x 1 11 x and 1 12 x If you are running builds provided by a distributor contact their support to determine which of their versions are vulnerable Some distributors may apply fixes to versions prior to the open source supported versions is Heapster considered aggregated API server same as metrics server No Heapster is not an aggregated API server Aggregated API servers register objects with non nil fields and requests to endpoints on the kube apiserver are proxied to the appropriate aggregated server Are there any detail procedures to reproduce the issue? So we can verify that if we have fixed it on 1 9 x Thanks a lot How to reproduce the issue so we can check if the kubernetes version we re using is affected by this issue? A pod exec attach portforward API call can be escalated to perform any API request against the kubelet API on the node specified in the pod spec An example script command shows how gets all pods information on that node can be helpful Is it enough to just update the API Server to a fixed version or should all the components be updated to the fixed Kubernetes version to be secure? fix is in the apiserver codebase so it should be enough to update just it Does this affect ELB ALB Ingress controllers in any way? I mean if K8s API is hidden in a private network but load balancers for services are wide open is it possible to use this vulnerability? Does this affect ELB ALB Ingress controllers in any way? I mean if K8s API is hidden in a private network but load balancers for services are wide open is it possible to use this vulnerability? If the request cannot reach the API servers the exploit cannot work Even if the request can reach the API servers but not have the permission to establish backend proxy connection it is also fine But if a user in your cluster can use kubectl it means that they can connect to K8s API server and the CVE may be applied here Or if the services that run on your cluster has a role that allows pod exec attach port forward it can proxy the request from the service to the API server Or if one of your services are API server extension and your users can interact with it directly then he she can craft the exploiting request Could someone give an example of an aggregated API endpoint that is usually enabled by default? Could someone give an example of an aggregated API endpoint that is usually enabled by default? the metrics server is an aggregated API server so is often a valid endpoint What is the safest way to and to ? Is it sufficient to move aside the proxy client key file mentioned here ? Don t laugh at this question is Kubernetes version 1 7 11 affected? I would assume so But if anyone can confirm this will help me out considerably Thanks James Sorry I won t add another comment I ll edit this one to thank you for your reply below I mis read the post on affected versions I did not see 1 0 x 1 9 x Sorry for wasting your time with the question Don t laugh at this question is Kubernetes version 1 7 11 affected? I would assume so But if anyone can confirm this will help me out considerably Thanks James I would say Yes as Kubernetes v1 0 x 1 9 x 1 7 x falls within that range Sorry to bug you but it would be a huge help to get an answer to my questions in I know the official docs on upgrading state to to from 1 7 to 1 8 then to 1 9 and then to 1 10 Would anyone know of a way to from 1 7 11 to 1 10 11 If not we will just figure out the steps and do the 3 separate upgrades If something similar has been done before we d like to hear about it as we have a few customers on 1 7 11 and all need to get upgraded We will of course do our own trial and error in a lab and try and script much of it An API call to any aggregated API server endpoint can be escalated to perform any API request against that aggregated API server as long as that aggregated API server is directly accessible from the Kubernetes API server s network In default configurations all users are allowed to perform discovery API calls that allow this escalation I don t understand why this leads to security issues what is the ? apatil What is the safest way to and to ? Without knowledge of your particular cluster and use that is difficult to say The safest way to suspend use of aggregated API servers might be to leave their objects registered and scale down the deployments backing those objects so that no aggregated server instances are available to receive requests Some potential disruptions that would cause If portions of the cluster depend on an aggregated server this would break those dependencies aggregated servers that cannot be reached cause the garbage collector and namespace cleanup controllers to wait to perform some operations until the unavailable server can be reached again This could prevent deletion of namespaces and prevent foreground deletion of objects until the servers are made available again Removing anonymous access would either consist of disabling anonymous requests entirely with or removing authorization permissions from anonymous users If using that includes removing the anonymous subjects from the and on restart Keep in mind that preventing anonymous access still leaves the cluster vulnerable to escalation by authenticated users apatil Is it sufficient to move aside the proxy client key file mentioned here ? Moving the would just prevent your kube apiserver from starting strouja I know the official docs on upgrading state to to from 1 7 to 1 8 then to 1 9 and then to 1 10 Would anyone know of a way to from 1 7 11 to 1 10 11 Skip version upgrading is not supported r00t4dm I don t understand why this leads to security issues what is the ? A low privilege request can be escalated to make any API call to the aggregated server The default configuration refers to the discovery permissions granted to all users by  
Request to hotfix 1 8 15 for CVE 2018 1002105 proxy request handling in kube apiserver We would need to see the CVE 2018 1002105 to be addressed with a hotfix in version 1 8 x Proxy request handling in kube apiserver can leave vulnerable TCP connections in version 1 8 15 This vulnerability to be addressed in the Kubernetes 1 8 core functionality of the api server Exec attach portforward permissions Referred to Kubernetes version 1 8 15 Cloud provider or hardware configuration AWS OS CoreOS Kernel 3 x Install tools KOPS Others DO NOT EDIT BELOW THIS LINE kind bug three minor versions of kubernetes are currently supported and receive bug and security fixes sig release this might be a good signal that it s time for you to update it was discussed not so long ago that by going outside of the maintenance skew a version no longer has a patch manager so one of the problems here is that there is no one to take the responsibility for merging patches for older branches sig release can decide to kindly ask the current on duty release team patch or branch manager to help but there are no guarantees cc does this only affect v1 8 x? no it affects multiple versions of Kubernetes I am concerned from responses from and that there might not be a follow up patch we are using KOPS in production and we still do not have a GA available for v1 11 x So bit stuck Any plans to indicate in release notes End of Life timelines for Kubernetes versions? does this only affect v1 8 x? the list of affected versions is as follows Kubernetes v1 0 x 1 9 x Kubernetes v1 10 0 1 10 10 Kubernetes v1 11 0 1 11 4 Kubernetes v1 12 0 1 12 2 we are using KOPS in production and we still do not have a GA available for v1 11 x v1 10 11 also contains the fix Any plans to indicate in release notes End of Life timelines for Kubernetes versions? Support for the most recent three minor versions has been the policy for the lifetime of the project That is currently documented at and is in the process of being added to the website in our customers need to know expect with what velocity of upgrades they need to progress and find the appropriate Kubernetes provisioning technology If you are using binaries provided by a distributor you should contact them to determine what versions resolve this CVE Distributors may choose to provide support for older releases beyond the ones maintained by the open source project close Closing this issue details In response to does this only affect v1 8 x? the list of affected versions is as follows Kubernetes v1 0 x 1 9 x Kubernetes v1 10 0 1 10 10 Kubernetes v1 11 0 1 11 4 Kubernetes v1 12 0 1 12 2 Any plans to indicate in release notes End of Life timelines for Kubernetes versions? Support for the most recent three minor versions has been the policy for the lifetime of the project That is currently documented at and is in the process of being added to the website in close Instructions for interacting with me using PR comments are available If you have questions or suggestions related to my behavior please file an issue against the repository details something you can do is to either mitigate or extract the patch for 1 10 and apply it to a custom k8s build because this is not the only critical fix that is missing in 1 8 We re on GKE The version is I assume this is the same with 1 10 11 which contains the fix? We re on GKE The version is I assume this is the same with 1 10 11 which contains the fix? If you are using binaries or packages provided by a distributor you should contact them to determine what versions resolve this CVE Distributors may choose to provide support for older releases beyond the ones maintained by the open source project In the case of GKE information is posted at I m relieved Thanks and thanks for responding here For the SIG Release perspective just to echo what was already said we intend to maintain the existing policy for support which is the three most recent minor versions No upstream patches will be provided for pre 1 10 releases Thank you all for the responses much appreciated It is more a challenge of a slow KOPS release cycle Example KOPS 1 11 is 7 months late with support of Kubernetes v1 11 x That means once KOPS will be releases we will have less than two months to enjoy a maintained Kubernetes version KOPS project should really consider being retired as velocity is not sufficient for secure commercial viable use anymore given the example of the vulnerability raised here  
